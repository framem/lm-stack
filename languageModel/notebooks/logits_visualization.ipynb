{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits Visualisierung - Sprachmodell\n",
    "\n",
    "Dieses Notebook zeigt die Logits und Wahrscheinlichkeiten des trainierten Sprachmodells als Heatmaps und Diagramme."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T19:51:48.439074800Z",
     "start_time": "2026-02-05T19:51:46.074868900Z"
    }
   },
   "source": "import sys\nfrom pathlib import Path\n\n# src-Verzeichnis zum Python-Path hinzufügen\nsrc_path = Path(\"..\").resolve()\nif str(src_path) not in sys.path:\n    sys.path.insert(0, str(src_path))\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importiere Modell-Klassen\nfrom training.training_lstm import load_model, Tokenizer, SimpleLanguageModel\n\n# Stil setzen\nplt.style.use('dark_background')\nsns.set_theme(style='darkgrid')\n\nprint(f\"Python-Path: {src_path}\")\nprint(\"Bibliotheken geladen!\")",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'training'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Importiere Modell-Klassen\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtraining\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining_lstm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model, Tokenizer, SimpleLanguageModel\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Stil setzen\u001B[39;00m\n\u001B[0;32m     19\u001B[0m plt\u001B[38;5;241m.\u001B[39mstyle\u001B[38;5;241m.\u001B[39muse(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdark_background\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'training'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Modell laden - Pfad relativ zur Notebook-Position\nmodel_dir = Path(\"../../dist/lstm_model\")\n\nif not model_dir.exists():\n    print(\"Modell nicht gefunden! Bitte zuerst trainieren:\")\n    print(\"  python src/main.py  (Option 1)\")\nelse:\n    model, tokenizer = load_model(str(model_dir))\n    print(f\"\\nVokabular: {tokenizer.vocab_size} Wörter\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logits für einen Text berechnen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_logits_and_probs(model, tokenizer, text):\n",
    "    \"\"\"Berechnet Logits und Wahrscheinlichkeiten für den nächsten Token.\"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "        logits = model(input_tensor)\n",
    "        last_logits = logits[0, -1, :]  # Nur letzter Token\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "    \n",
    "    return last_logits.numpy(), probs.numpy(), tokens\n",
    "\n",
    "# Test\n",
    "test_text = \"die katze\"\n",
    "logits, probs, tokens = get_logits_and_probs(model, tokenizer, test_text)\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Logits Shape: {logits.shape}\")\n",
    "print(f\"Probs Shape: {probs.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top-K Balkendiagramm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_top_k_bar(model, tokenizer, text, top_k=15):\n",
    "    \"\"\"Zeigt die Top-K wahrscheinlichsten Wörter als Balkendiagramm.\"\"\"\n",
    "    logits, probs, _ = get_logits_and_probs(model, tokenizer, text)\n",
    "    \n",
    "    # Top-K finden\n",
    "    top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "    top_probs = probs[top_indices]\n",
    "    top_words = [tokenizer.idx_to_word.get(i, \"<UNK>\") for i in top_indices]\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Wahrscheinlichkeiten\n",
    "    colors = plt.cm.viridis(np.linspace(0.8, 0.2, top_k))\n",
    "    bars1 = ax1.barh(range(top_k), top_probs * 100, color=colors)\n",
    "    ax1.set_yticks(range(top_k))\n",
    "    ax1.set_yticklabels(top_words)\n",
    "    ax1.set_xlabel('Wahrscheinlichkeit (%)')\n",
    "    ax1.set_title(f'Top-{top_k} Vorhersagen nach \"{text}\"')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Werte anzeigen\n",
    "    for bar, prob in zip(bars1, top_probs):\n",
    "        ax1.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                f'{prob*100:.1f}%', va='center', fontsize=9)\n",
    "    \n",
    "    # Logits\n",
    "    top_logits = logits[top_indices]\n",
    "    colors2 = ['green' if l > 0 else 'red' for l in top_logits]\n",
    "    bars2 = ax2.barh(range(top_k), top_logits, color=colors2, alpha=0.7)\n",
    "    ax2.set_yticks(range(top_k))\n",
    "    ax2.set_yticklabels(top_words)\n",
    "    ax2.set_xlabel('Logit-Wert')\n",
    "    ax2.set_title('Logits (vor Softmax)')\n",
    "    ax2.axvline(x=0, color='white', linestyle='--', alpha=0.5)\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_words, top_probs, top_logits\n",
    "\n",
    "# Visualisieren\n",
    "plot_top_k_bar(model, tokenizer, \"die katze\", top_k=12);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vollständige Logits-Heatmap (alle Wörter)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_full_logits_heatmap(model, tokenizer, text):\n",
    "    \"\"\"Zeigt alle Logits als Heatmap - ein Wert pro Wort im Vokabular.\"\"\"\n",
    "    logits, probs, _ = get_logits_and_probs(model, tokenizer, text)\n",
    "    \n",
    "    # Sortiere nach Wahrscheinlichkeit\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_logits = logits[sorted_indices]\n",
    "    sorted_words = [tokenizer.idx_to_word.get(i, \"?\")[:10] for i in sorted_indices]\n",
    "    \n",
    "    # Heatmap (als 2D-Matrix reshaped)\n",
    "    n_words = len(sorted_logits)\n",
    "    cols = 10\n",
    "    rows = (n_words + cols - 1) // cols\n",
    "    \n",
    "    # Padding falls nötig\n",
    "    padded_logits = np.pad(sorted_logits, (0, rows * cols - n_words), constant_values=np.nan)\n",
    "    matrix = padded_logits.reshape(rows, cols)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, max(6, rows * 0.5)))\n",
    "    \n",
    "    im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, label='Logit-Wert')\n",
    "    \n",
    "    # Labels\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i * cols + j\n",
    "            if idx < n_words:\n",
    "                word = sorted_words[idx][:6]\n",
    "                val = sorted_logits[idx]\n",
    "                color = 'white' if abs(val) > 1 else 'black'\n",
    "                ax.text(j, i, f\"{word}\\n{val:.1f}\", ha='center', va='center', \n",
    "                       fontsize=7, color=color)\n",
    "    \n",
    "    ax.set_title(f'Alle Logits für \"{text}\" (sortiert nach Wahrscheinlichkeit)')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_full_logits_heatmap(model, tokenizer, \"der hund\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generierungsprozess Schritt für Schritt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_generation_steps(model, tokenizer, start_text, max_steps=6, top_k=8):\n",
    "    \"\"\"Visualisiert jeden Schritt der Text-Generierung.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenizer.encode(start_text)\n",
    "    generated_words = start_text.split()\n",
    "    \n",
    "    # Sammle Daten für alle Schritte\n",
    "    all_probs = []\n",
    "    all_words = []\n",
    "    chosen_words = []\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for step in range(min(max_steps, 6)):\n",
    "        context = tokens[-5:] if len(tokens) > 5 else tokens\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor(context).unsqueeze(0)\n",
    "            logits = model(input_tensor)\n",
    "            last_logits = logits[0, -1, :]\n",
    "            probs = F.softmax(last_logits, dim=-1).numpy()\n",
    "        \n",
    "        # Top-K\n",
    "        top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "        top_probs = probs[top_indices]\n",
    "        top_words = [tokenizer.idx_to_word.get(i, \"?\") for i in top_indices]\n",
    "        \n",
    "        # Wähle nächstes Wort (greedy für Visualisierung)\n",
    "        next_token = top_indices[0]\n",
    "        next_word = top_words[0]\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[step]\n",
    "        colors = plt.cm.Blues(np.linspace(0.8, 0.3, top_k))\n",
    "        bars = ax.barh(range(top_k), top_probs * 100, color=colors)\n",
    "        ax.set_yticks(range(top_k))\n",
    "        ax.set_yticklabels(top_words)\n",
    "        ax.set_xlabel('Wahrsch. (%)')\n",
    "        \n",
    "        context_text = tokenizer.decode(context)\n",
    "        ax.set_title(f'Schritt {step+1}: \"{context_text}\" → ?', fontsize=10)\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Markiere gewähltes Wort\n",
    "        bars[0].set_color('lime')\n",
    "        \n",
    "        # EOS Check\n",
    "        if next_token == tokenizer.word_to_idx.get(tokenizer.eos_token):\n",
    "            ax.annotate('EOS!', xy=(0.5, 0.5), xycoords='axes fraction',\n",
    "                       fontsize=20, color='red', ha='center')\n",
    "            break\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "        generated_words.append(next_word)\n",
    "    \n",
    "    # Verstecke ungenutzte Subplots\n",
    "    for i in range(step + 1, 6):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Generierung: \"{\" \".join(generated_words)}\"', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "# Visualisieren\n",
    "result = visualize_generation_steps(model, tokenizer, \"die katze\", max_steps=6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vergleich mehrerer Eingaben"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_inputs_heatmap(model, tokenizer, texts, top_k=10):\n",
    "    \"\"\"Vergleicht die Top-K Vorhersagen für mehrere Eingaben als Heatmap.\"\"\"\n",
    "    \n",
    "    # Sammle alle Top-Wörter\n",
    "    all_top_words = set()\n",
    "    text_probs = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        _, probs, _ = get_logits_and_probs(model, tokenizer, text)\n",
    "        top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "        top_words = [tokenizer.idx_to_word.get(i, \"?\") for i in top_indices]\n",
    "        all_top_words.update(top_words)\n",
    "        text_probs[text] = {tokenizer.idx_to_word.get(i, \"?\"): probs[i] for i in top_indices}\n",
    "    \n",
    "    # Sortiere Wörter\n",
    "    all_top_words = sorted(all_top_words)\n",
    "    \n",
    "    # Erstelle Matrix\n",
    "    matrix = np.zeros((len(texts), len(all_top_words)))\n",
    "    for i, text in enumerate(texts):\n",
    "        for j, word in enumerate(all_top_words):\n",
    "            matrix[i, j] = text_probs[text].get(word, 0) * 100\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(max(12, len(all_top_words) * 0.8), len(texts) * 1.5 + 2))\n",
    "    \n",
    "    im = ax.imshow(matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, label='Wahrscheinlichkeit (%)')\n",
    "    \n",
    "    ax.set_xticks(range(len(all_top_words)))\n",
    "    ax.set_xticklabels(all_top_words, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(texts)))\n",
    "    ax.set_yticklabels([f'\"{t}\"' for t in texts])\n",
    "    \n",
    "    # Werte in Zellen\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(all_top_words)):\n",
    "            val = matrix[i, j]\n",
    "            if val > 0.5:\n",
    "                color = 'white' if val > 5 else 'black'\n",
    "                ax.text(j, i, f'{val:.1f}', ha='center', va='center', fontsize=8, color=color)\n",
    "    \n",
    "    ax.set_title('Vergleich: Welches Wort kommt nach...?')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vergleiche\n",
    "compare_inputs_heatmap(model, tokenizer, [\n",
    "    \"die katze\",\n",
    "    \"der hund\",\n",
    "    \"das kind\",\n",
    "    \"die sonne\"\n",
    "], top_k=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wahrscheinlichkeitsverteilung (Pie Chart)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_probability_pie(model, tokenizer, text, top_k=8):\n",
    "    \"\"\"Zeigt die Wahrscheinlichkeitsverteilung als Kreisdiagramm.\"\"\"\n",
    "    _, probs, _ = get_logits_and_probs(model, tokenizer, text)\n",
    "    \n",
    "    # Top-K + Rest\n",
    "    top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "    top_probs = probs[top_indices]\n",
    "    top_words = [tokenizer.idx_to_word.get(i, \"?\") for i in top_indices]\n",
    "    \n",
    "    rest_prob = 1.0 - sum(top_probs)\n",
    "    \n",
    "    labels = top_words + [f'Andere ({tokenizer.vocab_size - top_k} Wörter)']\n",
    "    sizes = list(top_probs) + [rest_prob]\n",
    "    \n",
    "    # Farben\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Pie Chart\n",
    "    explode = [0.05] * top_k + [0.1]\n",
    "    wedges, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "                                        explode=explode, colors=colors, \n",
    "                                        textprops={'fontsize': 9})\n",
    "    ax1.set_title(f'Wahrscheinlichkeitsverteilung nach \"{text}\"')\n",
    "    \n",
    "    # Kumulative Verteilung\n",
    "    cumsum = np.cumsum(top_probs)\n",
    "    ax2.fill_between(range(top_k), cumsum * 100, alpha=0.3, color='blue')\n",
    "    ax2.plot(range(top_k), cumsum * 100, 'bo-', markersize=8)\n",
    "    ax2.set_xticks(range(top_k))\n",
    "    ax2.set_xticklabels(top_words, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Kumulative Wahrscheinlichkeit (%)')\n",
    "    ax2.set_xlabel('Wort (sortiert nach Wahrsch.)')\n",
    "    ax2.set_title('Kumulative Verteilung (für Top-P Sampling)')\n",
    "    ax2.axhline(y=90, color='red', linestyle='--', label='Top-P=0.9')\n",
    "    ax2.axhline(y=50, color='orange', linestyle='--', label='Top-P=0.5')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_probability_pie(model, tokenizer, \"der hund\", top_k=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interaktive Eingabe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Eigenen Text eingeben und analysieren\n",
    "eingabe = \"die katze sitzt\"  # <-- Hier eigenen Text eingeben!\n",
    "\n",
    "print(f\"Analyse für: '{eingabe}'\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Alle Visualisierungen\n",
    "plot_top_k_bar(model, tokenizer, eingabe, top_k=10)\n",
    "plot_probability_pie(model, tokenizer, eingabe, top_k=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Temperatur-Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compare_temperatures(model, tokenizer, text, temperatures=[0.0, 0.5, 1.0, 1.5, 2.0], top_k=8):\n    \"\"\"Zeigt wie Temperature die Wahrscheinlichkeitsverteilung beeinflusst.\n    \n    Temperature = 0: Argmax (deterministisch, Top-Wort bekommt 100%)\n    Temperature = 1: Standard Softmax\n    Temperature > 1: Flachere Verteilung (mehr Zufall)\n    Temperature < 1: Schärfere Verteilung (weniger Zufall)\n    \"\"\"\n    logits, _, _ = get_logits_and_probs(model, tokenizer, text)\n    \n    # Top-K basierend auf originalen Logits (gleich für alle Temperaturen!)\n    top_indices = np.argsort(logits)[-top_k:][::-1]\n    top_words = [tokenizer.idx_to_word.get(i, \"?\")[:8] for i in top_indices]\n    \n    fig, axes = plt.subplots(1, len(temperatures), figsize=(4*len(temperatures), 5))\n    \n    for ax, temp in zip(axes, temperatures):\n        if temp == 0:\n            # Temperatur 0 = Argmax: Top-Wort bekommt 100%, Rest 0%\n            probs = np.zeros_like(logits)\n            probs[np.argmax(logits)] = 1.0\n        else:\n            # Logits mit Temperature skalieren\n            scaled_logits = logits / temp\n            probs = F.softmax(torch.tensor(scaled_logits), dim=-1).numpy()\n        \n        # Wahrscheinlichkeiten für die (gleichen) Top-K Wörter\n        top_probs = probs[top_indices]\n        \n        # Plot\n        colors = plt.cm.coolwarm(np.linspace(0.8, 0.2, top_k))\n        ax.barh(range(top_k), top_probs * 100, color=colors)\n        ax.set_yticks(range(top_k))\n        ax.set_yticklabels(top_words)\n        ax.set_xlabel('Wahrsch. (%)')\n        ax.set_title(f'Temp = {temp}')\n        ax.invert_yaxis()\n        ax.set_xlim(0, 100)\n    \n    plt.suptitle(f'Temperatur-Effekt auf \"{text}\"\\n(0 = argmax, niedriger = fokussierter, höher = zufälliger)', y=1.05)\n    plt.tight_layout()\n    plt.show()\n\ncompare_temperatures(model, tokenizer, \"die katze\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Logits über Sequenz (alle Positionen)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_sequence_logits(model, tokenizer, text, top_k=5):\n",
    "    \"\"\"Zeigt die Logits für jede Position in der Sequenz.\"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "        all_logits = model(input_tensor)[0]  # [seq_len, vocab_size]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(words), figsize=(4*len(words), 5))\n",
    "    if len(words) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for pos, (ax, word) in enumerate(zip(axes, words)):\n",
    "        logits = all_logits[pos].numpy()\n",
    "        probs = F.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        \n",
    "        top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "        top_probs = probs[top_indices]\n",
    "        top_words = [tokenizer.idx_to_word.get(i, \"?\")[:8] for i in top_indices]\n",
    "        \n",
    "        ax.barh(range(top_k), top_probs * 100, color=plt.cm.Greens(np.linspace(0.8, 0.3, top_k)))\n",
    "        ax.set_yticks(range(top_k))\n",
    "        ax.set_yticklabels(top_words)\n",
    "        ax.set_xlabel('Wahrsch. (%)')\n",
    "        ax.set_title(f'Nach \"{word}\"')\n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "    plt.suptitle(f'Vorhersagen an jeder Position: \"{text}\"', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sequence_logits(model, tokenizer, \"der hund frisst\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Embedding-Visualisierung\n\nEmbeddings sind hochdimensionale Vektoren (z.B. 32D), die jedes Wort im Vokabular repräsentieren.\nÄhnliche Wörter sollten ähnliche Vektoren haben und im Embedding-Raum nahe beieinander liegen.\n\n**Visualisierungsmethoden:**\n- **PCA** (Principal Component Analysis): Schnell, linear, zeigt globale Struktur\n- **t-SNE**: Langsamer, nicht-linear, zeigt Cluster besser\n\nDas Modell lernt diese Vektoren während des Trainings!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef plot_embeddings_pca(model, tokenizer, highlight_words=None, figsize=(12, 10)):\n    \"\"\"\n    Visualisiert die Embedding-Vektoren mit PCA (2D Projektion).\n    \n    Args:\n        model: Das trainierte Modell\n        tokenizer: Der Tokenizer mit dem Vokabular\n        highlight_words: Liste von Wörtern, die hervorgehoben werden sollen\n        figsize: Größe der Figur\n    \"\"\"\n    # Embedding-Gewichte extrahieren\n    embeddings = model.embedding.weight.detach().numpy()\n    vocab_size, embed_dim = embeddings.shape\n    \n    print(f\"Embedding-Matrix: {vocab_size} Wörter × {embed_dim} Dimensionen\")\n    \n    # PCA auf 2D reduzieren\n    pca = PCA(n_components=2)\n    embeddings_2d = pca.fit_transform(embeddings)\n    \n    print(f\"Erklärte Varianz: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n    print(f\"  - PC1: {pca.explained_variance_ratio_[0]*100:.1f}%\")\n    print(f\"  - PC2: {pca.explained_variance_ratio_[1]*100:.1f}%\")\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Alle Wörter als Punkte\n    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n               alpha=0.6, s=50, c='steelblue')\n    \n    # Wörter beschriften\n    for i in range(vocab_size):\n        word = tokenizer.idx_to_word.get(i, f\"[{i}]\")\n        x, y = embeddings_2d[i]\n        \n        # Hervorheben falls in Liste\n        if highlight_words and word in highlight_words:\n            ax.scatter([x], [y], s=200, c='red', marker='*', zorder=5)\n            ax.annotate(word, (x, y), fontsize=12, fontweight='bold',\n                       color='red', ha='center', va='bottom',\n                       xytext=(0, 8), textcoords='offset points')\n        else:\n            ax.annotate(word, (x, y), fontsize=8, alpha=0.7,\n                       ha='center', va='bottom')\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n    ax.set_title('Embedding-Visualisierung (PCA)')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return embeddings_2d, pca\n\n\ndef plot_embeddings_tsne(model, tokenizer, perplexity=5, figsize=(12, 10)):\n    \"\"\"\n    Visualisiert die Embedding-Vektoren mit t-SNE (2D Projektion).\n    t-SNE ist besser für Cluster, aber langsamer und nicht deterministisch.\n    \n    Args:\n        model: Das trainierte Modell\n        tokenizer: Der Tokenizer mit dem Vokabular\n        perplexity: t-SNE Parameter (5-50, kleiner = lokaler Fokus)\n        figsize: Größe der Figur\n    \"\"\"\n    # Embedding-Gewichte extrahieren\n    embeddings = model.embedding.weight.detach().numpy()\n    vocab_size, embed_dim = embeddings.shape\n    \n    print(f\"Embedding-Matrix: {vocab_size} Wörter × {embed_dim} Dimensionen\")\n    print(f\"t-SNE mit perplexity={perplexity}...\")\n    \n    # t-SNE auf 2D reduzieren\n    tsne = TSNE(n_components=2, perplexity=min(perplexity, vocab_size-1), \n                random_state=42, n_iter=1000)\n    embeddings_2d = tsne.fit_transform(embeddings)\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Farben nach Wortart (einfache Heuristik)\n    colors = []\n    for i in range(vocab_size):\n        word = tokenizer.idx_to_word.get(i, \"\")\n        if word.startswith(\"<\"):  # Special tokens\n            colors.append('gray')\n        elif word in ['der', 'die', 'das', 'den', 'dem', 'ein', 'eine']:\n            colors.append('green')  # Artikel\n        elif word in ['auf', 'in', 'im', 'am', 'vom', 'mit', 'über', 'neben']:\n            colors.append('orange')  # Präpositionen\n        else:\n            colors.append('steelblue')  # Rest\n    \n    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n               alpha=0.7, s=80, c=colors)\n    \n    # Wörter beschriften\n    for i in range(vocab_size):\n        word = tokenizer.idx_to_word.get(i, f\"[{i}]\")\n        x, y = embeddings_2d[i]\n        ax.annotate(word, (x, y), fontsize=8, alpha=0.8,\n                   ha='center', va='bottom')\n    \n    ax.set_xlabel('t-SNE Dimension 1')\n    ax.set_ylabel('t-SNE Dimension 2')\n    ax.set_title('Embedding-Visualisierung (t-SNE)')\n    ax.grid(True, alpha=0.3)\n    \n    # Legende\n    from matplotlib.patches import Patch\n    legend_elements = [\n        Patch(facecolor='green', label='Artikel'),\n        Patch(facecolor='orange', label='Präpositionen'),\n        Patch(facecolor='steelblue', label='Andere'),\n        Patch(facecolor='gray', label='Special Tokens')\n    ]\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return embeddings_2d\n\n\ndef plot_embedding_similarity(model, tokenizer, words, figsize=(10, 8)):\n    \"\"\"\n    Zeigt die Kosinus-Ähnlichkeit zwischen ausgewählten Wörtern als Heatmap.\n    \n    Args:\n        model: Das trainierte Modell\n        tokenizer: Der Tokenizer\n        words: Liste von Wörtern zum Vergleichen\n    \"\"\"\n    # Embedding-Vektoren für die Wörter holen\n    embeddings = model.embedding.weight.detach().numpy()\n    \n    word_indices = []\n    valid_words = []\n    for word in words:\n        idx = tokenizer.word_to_idx.get(word.lower())\n        if idx is not None:\n            word_indices.append(idx)\n            valid_words.append(word)\n        else:\n            print(f\"Warnung: '{word}' nicht im Vokabular\")\n    \n    if len(valid_words) < 2:\n        print(\"Mindestens 2 gültige Wörter benötigt!\")\n        return\n    \n    # Embedding-Vektoren extrahieren\n    word_embeddings = embeddings[word_indices]\n    \n    # Kosinus-Ähnlichkeit berechnen\n    # cos_sim(a, b) = (a · b) / (||a|| * ||b||)\n    norms = np.linalg.norm(word_embeddings, axis=1, keepdims=True)\n    normalized = word_embeddings / norms\n    similarity_matrix = normalized @ normalized.T\n    \n    # Heatmap\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    im = ax.imshow(similarity_matrix, cmap='RdYlGn', vmin=-1, vmax=1)\n    plt.colorbar(im, ax=ax, label='Kosinus-Ähnlichkeit')\n    \n    ax.set_xticks(range(len(valid_words)))\n    ax.set_yticks(range(len(valid_words)))\n    ax.set_xticklabels(valid_words, rotation=45, ha='right')\n    ax.set_yticklabels(valid_words)\n    \n    # Werte in Zellen\n    for i in range(len(valid_words)):\n        for j in range(len(valid_words)):\n            val = similarity_matrix[i, j]\n            color = 'white' if abs(val) > 0.5 else 'black'\n            ax.text(j, i, f'{val:.2f}', ha='center', va='center', \n                   fontsize=10, color=color)\n    \n    ax.set_title('Embedding-Ähnlichkeit (Kosinus)')\n    plt.tight_layout()\n    plt.show()\n    \n    return similarity_matrix, valid_words\n\n\nprint(\"Embedding-Visualisierungsfunktionen geladen!\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# PCA Visualisierung - alle Wörter im 2D-Raum\n",
    "# Ähnliche Wörter sollten nahe beieinander liegen\n",
    "plot_embeddings_pca(model, tokenizer, highlight_words=['katze', 'hund', 'die', 'der', 'kinder'])"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# t-SNE Visualisierung - zeigt Cluster besser\n# Farben: Grün=Artikel, Orange=Präpositionen, Blau=Rest\nplot_embeddings_tsne(model, tokenizer, perplexity=8)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Ähnlichkeits-Heatmap für ausgewählte Wörter\n# Wert 1.0 = identisch, 0 = unkorreliert, -1 = entgegengesetzt\nplot_embedding_similarity(model, tokenizer, [\n    'katze', 'hund', 'kind',      # Subjekte\n    'sitzt', 'läuft', 'spielt',   # Verben\n    'die', 'der', 'das',          # Artikel\n    'auf', 'im', 'am'             # Präpositionen\n])",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
