{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge: Automatische Qualitaetsbewertung\n",
    "\n",
    "## Was ist LLM-as-a-Judge?\n",
    "\n",
    "Wenn man Sprachmodelle trainiert, stellt sich die Frage: **Wie gut sind die generierten Texte?**\n",
    "\n",
    "Es gibt drei Ansaetze:\n",
    "\n",
    "| Ansatz | Vorteile | Nachteile |\n",
    "|--------|----------|-----------|\n",
    "| **Menschliche Bewertung** | Goldstandard, nuanciert | Langsam, teuer, nicht skalierbar |\n",
    "| **Automatische Metriken** (BLEU, ROUGE) | Schnell, reproduzierbar | Korrelieren schlecht mit Qualitaet |\n",
    "| **LLM-as-a-Judge** | Schnell, nuanciert, skalierbar | Abhaengig vom Judge-Modell, potenzielle Biases |\n",
    "\n",
    "In diesem Notebook verwenden wir ein **grosses lokales LLM** (z.B. Qwen3 via Ollama/LM Studio),\n",
    "um die Outputs unserer kleinen MiniGPT-Modelle zu bewerten.\n",
    "\n",
    "### Bewertungskriterien\n",
    "\n",
    "Jeder generierte Text wird auf drei Kriterien bewertet (je 1-5):\n",
    "1. **Grammatik**: Korrektheit der Satzstruktur\n",
    "2. **Kohaerenz**: Logischer Zusammenhang\n",
    "3. **Relevanz**: Bezug zum Eingabe-Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Voraussetzungen:**\n",
    "- Trainierte Modelle (mindestens `python src/main.py` -> Option 2)\n",
    "- Ollama oder LM Studio laeuft lokal mit einem Modell (z.B. `qwen3:8b`)\n",
    "- `pip install openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# src-Verzeichnis zum Python-Path hinzufuegen\n",
    "src_path = Path(\"../src\").resolve()\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Stil setzen\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({'axes.grid': True, 'grid.alpha': 0.3})\n",
    "\n",
    "print(f\"Python-Path: {src_path}\")\n",
    "print(\"Bibliotheken geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Der Judge-Prompt\n",
    "\n",
    "Das Herztueck von LLM-as-a-Judge ist der **System-Prompt**, der dem Judge-LLM erklaert,\n",
    "wie es bewerten soll. Wichtig ist dabei:\n",
    "\n",
    "- **Kontext geben**: Das Judge-LLM muss wissen, dass die Texte von einem winzigen Modell stammen\n",
    "- **Klare Skala**: Jeder Score-Wert (1-5) wird explizit definiert\n",
    "- **Strukturierte Ausgabe**: JSON-Format erzwingen fuer zuverlaessiges Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.judge import JUDGE_SYSTEM_PROMPT, JUDGE_USER_PROMPT_TEMPLATE\n",
    "\n",
    "print(\"=== SYSTEM PROMPT ===\")\n",
    "print(JUDGE_SYSTEM_PROMPT)\n",
    "print()\n",
    "print(\"=== USER PROMPT TEMPLATE ===\")\n",
    "print(JUDGE_USER_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Modelle laden\n",
    "\n",
    "Wir verwenden die gleichen Funktionen wie in der Inferenz, um alle\n",
    "verfuegbaren Modelle (Original + Fine-Tuned) zu finden und zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.inference_finetuned import discover_models, load_model_by_type, generate_text\n",
    "\n",
    "base_dir = Path(\"../dist\")\n",
    "available = discover_models(base_dir)\n",
    "\n",
    "print(f\"Gefundene Modelle ({len(available)}):\\n\")\n",
    "for name, info in available.items():\n",
    "    print(f\"  - {name:<20} {info['label']}\")\n",
    "\n",
    "# Alle Modelle laden\n",
    "loaded_models = {}\n",
    "for name, info in available.items():\n",
    "    try:\n",
    "        print(f\"\\nLade {info['label']}...\")\n",
    "        model, tokenizer = load_model_by_type(info, base_dir)\n",
    "        loaded_models[name] = (model, tokenizer, info['label'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Fehler: {e}\")\n",
    "\n",
    "print(f\"\\n{len(loaded_models)} Modell(e) geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Einzelbeispiel: Ein Prompt bewerten\n",
    "\n",
    "Bevor wir alle Modelle systematisch bewerten, schauen wir uns\n",
    "einen einzelnen Durchlauf an: **Prompt -> Generierung -> Judge -> Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.llm_client import test_connection, get_model_name\n",
    "from evaluation.judge import evaluate_single_output\n",
    "\n",
    "# LLM-Verbindung pruefen\n",
    "print(f\"Judge-Modell: {get_model_name()}\")\n",
    "print(f\"Verbindung: {'OK' if test_connection() else 'FEHLGESCHLAGEN'}\")\n",
    "\n",
    "# Erstes Modell nehmen\n",
    "first_key = list(loaded_models.keys())[0]\n",
    "model, tokenizer, label = loaded_models[first_key]\n",
    "\n",
    "# Text generieren\n",
    "test_prompt = \"die katze\"\n",
    "generated = generate_text(model, tokenizer, test_prompt, max_length=8, temperature=0.8)\n",
    "\n",
    "print(f\"\\nModell:     {label}\")\n",
    "print(f\"Prompt:     '{test_prompt}'\")\n",
    "print(f\"Generiert:  '{generated}'\")\n",
    "\n",
    "# Judge bewerten lassen\n",
    "score = evaluate_single_output(test_prompt, generated)\n",
    "\n",
    "if score:\n",
    "    print(f\"\\n--- Bewertung ---\")\n",
    "    print(f\"Grammatik:  {score.grammatik_score}/5\")\n",
    "    print(f\"Kohaerenz:  {score.kohaerenz_score}/5\")\n",
    "    print(f\"Relevanz:   {score.relevanz_score}/5\")\n",
    "    print(f\"Gesamt:     {score.gesamt_score}/5\")\n",
    "    print(f\"Begruendung: {score.begruendung}\")\n",
    "else:\n",
    "    print(\"\\nBewertung fehlgeschlagen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Systematische Bewertung: Alle Modelle x Alle Prompts\n",
    "\n",
    "Jetzt bewerten wir **jedes Modell** auf **jedem Prompt** und sammeln die Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.judge_config import (\n",
    "    DEFAULT_TEST_PROMPTS,\n",
    "    GENERATION_TEMPERATURE,\n",
    "    GENERATION_MAX_LENGTH,\n",
    "    GeneratedOutput,\n",
    "    ModelEvaluationResult,\n",
    ")\n",
    "\n",
    "prompts = DEFAULT_TEST_PROMPTS\n",
    "print(f\"Prompts: {prompts}\")\n",
    "print(f\"Temperature: {GENERATION_TEMPERATURE}\")\n",
    "print(f\"Max. Laenge: {GENERATION_MAX_LENGTH}\")\n",
    "print(f\"Modelle: {len(loaded_models)}\")\n",
    "print(f\"Gesamt-Bewertungen: {len(prompts) * len(loaded_models)}\")\n",
    "print()\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_key, (model, tokenizer, label) in loaded_models.items():\n",
    "    model_result = ModelEvaluationResult(model_name=model_key, label=label)\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = generate_text(\n",
    "            model, tokenizer, prompt,\n",
    "            max_length=GENERATION_MAX_LENGTH,\n",
    "            temperature=GENERATION_TEMPERATURE,\n",
    "        )\n",
    "        \n",
    "        score = evaluate_single_output(prompt, generated)\n",
    "        output = GeneratedOutput(\n",
    "            model_name=model_key,\n",
    "            prompt=prompt,\n",
    "            generated_text=generated,\n",
    "            score=score,\n",
    "        )\n",
    "        model_result.outputs.append(output)\n",
    "        \n",
    "        score_str = f\"G:{score.grammatik_score} K:{score.kohaerenz_score} R:{score.relevanz_score} = {score.gesamt_score:.2f}\" if score else \"FEHLER\"\n",
    "        print(f\"  '{prompt}' -> '{generated}' [{score_str}]\")\n",
    "    \n",
    "    all_results.append(model_result)\n",
    "\n",
    "print(\"\\nBewertung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Ergebnis-Tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking\n",
    "ranked = sorted(all_results, key=lambda r: r.avg_gesamt, reverse=True)\n",
    "\n",
    "print(f\"{'Rang':<6} {'Modell':<28} {'Gesamt':>8} {'Gram.':>8} {'Koh.':>8} {'Rel.':>8}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for i, r in enumerate(ranked, 1):\n",
    "    print(\n",
    "        f\"{i:<6} {r.label:<28} {r.avg_gesamt:>8.2f} \"\n",
    "        f\"{r.avg_grammatik:>8.2f} {r.avg_kohaerenz:>8.2f} {r.avg_relevanz:>8.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Visualisierung: Radar-Chart\n",
    "\n",
    "Ein Radar-Chart zeigt die drei Kriterien pro Modell auf einen Blick.\n",
    "Groessere Flaeche = besseres Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(results):\n",
    "    \"\"\"Radar-Chart: 3 Kriterien pro Modell.\"\"\"\n",
    "    categories = ['Grammatik', 'Kohaerenz', 'Relevanz']\n",
    "    N = len(categories)\n",
    "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Schliessen\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(results)))\n",
    "\n",
    "    for i, r in enumerate(results):\n",
    "        values = [r.avg_grammatik, r.avg_kohaerenz, r.avg_relevanz]\n",
    "        values += values[:1]  # Schliessen\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=r.label, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.15, color=colors[i])\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontsize=12)\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.set_yticks([1, 2, 3, 4, 5])\n",
    "    ax.set_title('Modellvergleich: Radar-Chart', fontsize=14, pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_radar_chart(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Visualisierung: Grouped Bar Chart\n",
    "\n",
    "Direkter Vergleich der Scores pro Kriterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_bar(results):\n",
    "    \"\"\"Grouped Bar Chart: Kriterien nebeneinander pro Modell.\"\"\"\n",
    "    labels = [r.label for r in results]\n",
    "    grammatik = [r.avg_grammatik for r in results]\n",
    "    kohaerenz = [r.avg_kohaerenz for r in results]\n",
    "    relevanz = [r.avg_relevanz for r in results]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, grammatik, width, label='Grammatik', color='#e74c3c')\n",
    "    ax.bar(x, kohaerenz, width, label='Kohaerenz', color='#3498db')\n",
    "    ax.bar(x + width, relevanz, width, label='Relevanz', color='#2ecc71')\n",
    "\n",
    "    ax.set_xlabel('Modell')\n",
    "    ax.set_ylabel('Score (1-5)')\n",
    "    ax.set_title('LLM-as-a-Judge: Score-Vergleich')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=20, ha='right')\n",
    "    ax.set_ylim(0, 5.5)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grouped_bar(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Visualisierung: Heatmap (Prompt x Modell)\n",
    "\n",
    "Zeigt den Gesamtscore fuer jede Kombination aus Prompt und Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(results, prompts):\n",
    "    \"\"\"Heatmap: Gesamtscore pro Prompt x Modell.\"\"\"\n",
    "    model_labels = [r.label for r in results]\n",
    "    \n",
    "    # Matrix aufbauen\n",
    "    matrix = np.zeros((len(prompts), len(results)))\n",
    "    for j, r in enumerate(results):\n",
    "        for i, o in enumerate(r.outputs):\n",
    "            if o.score:\n",
    "                matrix[i, j] = o.score.gesamt_score\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    im = ax.imshow(matrix, cmap='RdYlGn', vmin=1, vmax=5, aspect='auto')\n",
    "    \n",
    "    ax.set_xticks(range(len(model_labels)))\n",
    "    ax.set_xticklabels(model_labels, rotation=20, ha='right')\n",
    "    ax.set_yticks(range(len(prompts)))\n",
    "    ax.set_yticklabels(prompts)\n",
    "    ax.set_xlabel('Modell')\n",
    "    ax.set_ylabel('Prompt')\n",
    "    ax.set_title('Gesamtscore: Prompt x Modell')\n",
    "    \n",
    "    # Werte in Zellen schreiben\n",
    "    for i in range(len(prompts)):\n",
    "        for j in range(len(results)):\n",
    "            val = matrix[i, j]\n",
    "            color = 'black' if val > 3 else 'white'\n",
    "            ax.text(j, i, f'{val:.1f}', ha='center', va='center', color=color, fontsize=11)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Score (1-5)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_heatmap(all_results, DEFAULT_TEST_PROMPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Bias-Analyse: Grenzen von LLM-as-a-Judge\n",
    "\n",
    "LLM-as-a-Judge ist maechtig, aber nicht perfekt. Bekannte Probleme:\n",
    "\n",
    "### 1. Laengenbias (Length Bias)\n",
    "Grosse LLMs bevorzugen oft **laengere Antworten**, auch wenn kuerzere praeziser waeren.\n",
    "Bei unseren MiniGPT-Modellen ist das weniger relevant (alle generieren ~5-10 Woerter),\n",
    "aber bei groesseren Modellen ein ernstes Problem.\n",
    "\n",
    "### 2. Selbstbevorzugung (Self-Enhancement Bias)\n",
    "Ein LLM bewertet Texte, die seinem eigenen Stil aehneln, tendenziell besser.\n",
    "Da unsere MiniGPT-Texte sehr anders sind als typische LLM-Outputs,\n",
    "ist dieser Bias hier weniger ausgepraegt.\n",
    "\n",
    "### 3. Positions-Bias\n",
    "Wenn mehrere Texte verglichen werden, bevorzugen LLMs oft den **ersten** oder **letzten** Text.\n",
    "Wir vermeiden das, indem wir jeden Text **einzeln** bewerten.\n",
    "\n",
    "### 4. Kalibrierungsprobleme\n",
    "Verschiedene Judge-Modelle koennen stark unterschiedliche Scores geben.\n",
    "Ein Qwen3:8b bewertet moeglicherweise strenger als ein GPT-4.\n",
    "Deshalb sind **relative Rankings** zuverlaessiger als absolute Scores.\n",
    "\n",
    "### Gegenmassnahmen\n",
    "\n",
    "| Problem | Gegenmassnahme |\n",
    "|---------|----------------|\n",
    "| Laengenbias | Einzelbewertung statt Vergleich |\n",
    "| Selbstbevorzugung | Lokales Open-Source-Modell als Judge |\n",
    "| Kalibrierung | Kalibrierter Prompt mit expliziter Skala |\n",
    "| Rauschen | Mehrere Durchlaeufe, Mittelwert bilden |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Report generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation_report import generate_evaluation_report\n",
    "\n",
    "report_dir = Path(\"../dist/evaluation_results\")\n",
    "report = generate_evaluation_report(all_results, report_dir)\n",
    "\n",
    "print(\"Report gespeichert!\")\n",
    "print(f\"Pfad: {report_dir / 'EVALUATION_REPORT.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "### Was wir gelernt haben\n",
    "\n",
    "- **LLM-as-a-Judge** ermoeglicht automatisierte, nuancierte Textbewertung\n",
    "- Ein grosses LLM (Judge) bewertet die Outputs kleiner Modelle auf definierten Kriterien\n",
    "- Strukturierte Prompts mit JSON-Ausgabe machen die Ergebnisse maschinell auswertbar\n",
    "- Verschiedene Fine-Tuning-Methoden fuehren zu unterschiedlicher Textqualitaet\n",
    "\n",
    "### Wann LLM-as-a-Judge nutzen?\n",
    "\n",
    "| Situation | Empfehlung |\n",
    "|-----------|------------|\n",
    "| Schnelle Iteration waehrend der Entwicklung | LLM-as-a-Judge |\n",
    "| Vergleich vieler Modellvarianten | LLM-as-a-Judge |\n",
    "| Finale Qualitaetssicherung | Menschliche Bewertung |\n",
    "| Reproduzierbare Benchmark-Scores | Automatische Metriken (BLEU, etc.) |\n",
    "| Sicherheits-/Bias-Pruefung | Kombination aus LLM-Judge + Mensch |\n",
    "\n",
    "### Limitationen\n",
    "\n",
    "- Ergebnisse haengen stark vom Judge-Modell ab\n",
    "- Absolute Scores sind weniger aussagekraeftig als relative Rankings\n",
    "- Fuer produktive Systeme sollte immer auch menschliche Evaluation stattfinden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
