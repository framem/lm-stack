{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits Visualisierung - Sprachmodell\n",
    "\n",
    "Dieses Notebook zeigt die Logits und Wahrscheinlichkeiten des trainierten Sprachmodells als Heatmaps und Diagramme."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nfrom pathlib import Path\n\n# src-Verzeichnis zum Python-Path hinzufügen\nsrc_path = Path(\"../src\").resolve()\nif str(src_path) not in sys.path:\n    sys.path.insert(0, str(src_path))\n\nimport json\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Importiere beide Modell-Typen\nfrom training.training_lstm import load_model as load_lstm_model, Tokenizer, SimpleLanguageModel\nfrom training.training_transformer import load_transformer_model, MiniGPT, SimpleTokenizer\n\n# Stil setzen\nplt.style.use('dark_background')\nplt.rcParams.update({'axes.grid': True, 'grid.alpha': 0.3})\n\nprint(f\"Python-Path: {src_path}\")\nprint(\"Bibliotheken geladen (LSTM + Transformer)!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Modell laden - LSTM oder Transformer automatisch erkennen\n",
    "# ============================================================\n",
    "# Zum Wechseln einfach den Pfad ändern:\n",
    "#   LSTM:        model_dir = Path(\"../dist/lstm_model\")\n",
    "#   Transformer: model_dir = Path(\"../dist/transformer_model\")\n",
    "# ============================================================\n",
    "\n",
    "model_dir = Path(\"../dist/transformer_model\")\n",
    "\n",
    "if not model_dir.exists():\n",
    "    print(f\"Modell nicht gefunden unter: {model_dir}\")\n",
    "    print(\"  Bitte zuerst trainieren: python src/main.py\")\n",
    "else:\n",
    "    # Model-Typ aus config.json erkennen\n",
    "    with open(model_dir / \"config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    model_type = config.get(\"model_type\", \"LSTM\")\n",
    "    \n",
    "    if model_type == \"MiniGPT\":\n",
    "        model, tokenizer = load_transformer_model(str(model_dir))\n",
    "        # Kompatibilität: embedding-Zugriff vereinheitlichen\n",
    "        model.embedding = model.token_embedding\n",
    "        # Kompatibilität: eos_token für Generierungs-Visualisierung\n",
    "        tokenizer.eos_token = \"<EOS>\"\n",
    "        print(f\"\\n[Transformer] Vokabular: {tokenizer.vocab_size} Wörter\")\n",
    "    else:\n",
    "        model, tokenizer = load_lstm_model(str(model_dir))\n",
    "        print(f\"\\n[LSTM] Vokabular: {tokenizer.vocab_size} Wörter\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logits für einen Text berechnen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def get_logits_and_probs(model, tokenizer, text):\n    \"\"\"Berechnet Logits und Wahrscheinlichkeiten für den nächsten Token.\"\"\"\n    model.eval()\n    tokens = tokenizer.encode(text)\n    \n    with torch.no_grad():\n        input_tensor = torch.tensor(tokens).unsqueeze(0)\n        logits = model(input_tensor)\n        last_logits = logits[0, -1, :]  # Nur letzter Token\n        probs = F.softmax(last_logits, dim=-1)\n    \n    return last_logits.numpy(), probs.numpy(), tokens\n\n# Test\ntest_text = \"die katze\"\nlogits, probs, tokens = get_logits_and_probs(model, tokenizer, test_text)\nembed_dim = model.embedding.weight.shape[1]\n\n# --- Visualisierung statt nur print ---\nfig, axes = plt.subplots(1, 3, figsize=(16, 4),\n                         gridspec_kw={'width_ratios': [1.2, 1.5, 1.5]})\n\n# 1. Tokenisierung als farbige Boxen\nax1 = axes[0]\nax1.set_xlim(0, 10)\nax1.set_ylim(0, 4)\nax1.set_title('Tokenisierung', fontsize=11, fontweight='bold')\nwords = test_text.split()\nbox_colors = plt.cm.Set2(np.linspace(0, 0.5, len(words)))\nfor i, (word, tok_id) in enumerate(zip(words, tokens)):\n    x = 1 + i * 4\n    # Wort-Box\n    rect = plt.Rectangle((x, 2.2), 3, 1.2, linewidth=2,\n                          edgecolor='white', facecolor=box_colors[i], alpha=0.8)\n    ax1.add_patch(rect)\n    ax1.text(x + 1.5, 2.8, f'\"{word}\"', ha='center', va='center',\n             fontsize=12, fontweight='bold', color='black')\n    # Pfeil\n    ax1.annotate('', xy=(x + 1.5, 1.8), xytext=(x + 1.5, 2.2),\n                 arrowprops=dict(arrowstyle='->', color='white', lw=1.5))\n    # Token-ID Box mit Vocab-Info\n    rect2 = plt.Rectangle((x, 0.6), 3, 1.2, linewidth=2,\n                           edgecolor='white', facecolor='#2c3e50', alpha=0.9)\n    ax1.add_patch(rect2)\n    ax1.text(x + 1.5, 1.35, f'vocab[{tok_id}]', ha='center', va='center',\n             fontsize=11, color='#3498db', fontweight='bold')\n    ax1.text(x + 1.5, 0.95, f'→ {embed_dim}D Vektor', ha='center', va='center',\n             fontsize=8, color='#7f8c8d')\nax1.axis('off')\n\n# 2. Top-5 Vorhersagen (Mini-Balkendiagramm)\nax2 = axes[1]\ntop_k = 5\ntop_idx = np.argsort(probs)[-top_k:][::-1]\ntop_p = probs[top_idx]\ntop_w = [tokenizer.idx_to_word.get(i, \"?\") for i in top_idx]\ncolors = plt.cm.viridis(np.linspace(0.8, 0.3, top_k))\nbars = ax2.barh(range(top_k), top_p * 100, color=colors, edgecolor='white', linewidth=0.5)\nax2.set_yticks(range(top_k))\nax2.set_yticklabels(top_w, fontsize=11)\nax2.set_xlabel('Wahrscheinlichkeit (%)')\nax2.set_title(f'Top-5 nach \"{test_text}\" → ?', fontsize=11, fontweight='bold')\nax2.invert_yaxis()\nfor bar, p in zip(bars, top_p):\n    ax2.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n             f'{p*100:.1f}%', va='center', fontsize=10)\n\n# 3. Logit-Verteilung (Histogramm)\nax3 = axes[2]\nax3.hist(logits, bins=25, color='#3498db', alpha=0.7, edgecolor='white', linewidth=0.5)\nax3.axvline(x=logits.mean(), color='#e74c3c', linestyle='--', linewidth=2, label=f'Mean: {logits.mean():.2f}')\nax3.axvline(x=logits.max(), color='#2ecc71', linestyle='--', linewidth=2, label=f'Max: {logits.max():.2f}')\nax3.set_xlabel('Logit-Wert')\nax3.set_ylabel('Anzahl Wörter')\nax3.set_title(f'Logit-Verteilung ({len(logits)} Wörter)', fontsize=11, fontweight='bold')\nax3.legend(fontsize=9)\n\nplt.suptitle(f'Text → Tokens → Logits → Vorhersage', fontsize=13, y=1.03)\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top-K Balkendiagramm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def plot_top_k_bar(model, tokenizer, text, top_k=15):\n    \"\"\"Zeigt die Top-K wahrscheinlichsten Wörter als Balkendiagramm.\n    \n    Visualisiert die komplette Pipeline:\n    Logits (roh) → Softmax → Wahrscheinlichkeiten (%) → Kumulative Verteilung\n    \"\"\"\n    logits, probs, _ = get_logits_and_probs(model, tokenizer, text)\n    \n    # Top-K finden\n    top_indices = np.argsort(probs)[-top_k:][::-1]\n    top_probs = probs[top_indices]\n    top_logits = logits[top_indices]\n    top_words = [tokenizer.idx_to_word.get(i, \"<UNK>\") for i in top_indices]\n    \n    # 4 Subplots: Logits → Softmax → Wahrscheinlichkeiten → Kumulativ\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 6))\n    \n    # --- 1. Logits (vor Softmax) ---\n    colors_logits = ['#2ecc71' if l > 0 else '#e74c3c' for l in top_logits]\n    bars1 = ax1.barh(range(top_k), top_logits, color=colors_logits, alpha=0.7)\n    ax1.set_yticks(range(top_k))\n    ax1.set_yticklabels(top_words)\n    ax1.set_xlabel('Logit-Wert (unbegrenzt)')\n    ax1.set_title('1. Logits')\n    ax1.axvline(x=0, color='white', linestyle='--', alpha=0.5)\n    ax1.invert_yaxis()\n    \n    for bar, val in zip(bars1, top_logits):\n        x_pos = bar.get_width() + 0.1 if val >= 0 else bar.get_width() - 0.1\n        ha = 'left' if val >= 0 else 'right'\n        ax1.text(x_pos, bar.get_y() + bar.get_height()/2,\n                f'{val:.2f}', va='center', ha=ha, fontsize=9)\n    \n    # --- 2. Softmax-Werte (nach Softmax) ---\n    colors_softmax = plt.cm.Blues(np.linspace(0.8, 0.3, top_k))\n    bars2 = ax2.barh(range(top_k), top_probs, color=colors_softmax)\n    ax2.set_yticks(range(top_k))\n    ax2.set_yticklabels(top_words)\n    ax2.set_xlabel('Softmax-Wert (0 bis 1, Summe = 1.0)')\n    ax2.set_title('2. Softmax-Werte (Logits \\u2192 Wahrscheinlichkeiten)')\n    ax2.set_xlim(0, min(1.0, top_probs[0] * 1.3))\n    ax2.invert_yaxis()\n    \n    for bar, val in zip(bars2, top_probs):\n        ax2.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n                f'{val:.4f}', va='center', fontsize=9)\n    \n    # --- 3. Wahrscheinlichkeiten (%) ---\n    colors_prob = plt.cm.viridis(np.linspace(0.8, 0.2, top_k))\n    bars3 = ax3.barh(range(top_k), top_probs * 100, color=colors_prob)\n    ax3.set_yticks(range(top_k))\n    ax3.set_yticklabels(top_words)\n    ax3.set_xlabel('Wahrscheinlichkeit 0 bis 100%, Summe = 100.0%')\n    ax3.set_title('3. Wahrscheinlichkeiten')\n    ax3.invert_yaxis()\n    \n    for bar, prob in zip(bars3, top_probs):\n        ax3.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n                f'{prob*100:.1f}%', va='center', fontsize=9)\n    \n    # --- 4. Kumulative Verteilung ---\n    cumsum = np.cumsum(top_probs)\n    ax4.fill_between(range(top_k), cumsum * 100, alpha=0.3, color='blue')\n    ax4.plot(range(top_k), cumsum * 100, 'bo-', markersize=6)\n    ax4.set_xticks(range(top_k))\n    ax4.set_xticklabels(top_words, rotation=45, ha='right', fontsize=8)\n    ax4.set_ylabel('Kumulative Wahrsch. (%)')\n    ax4.set_xlabel('Wort (sortiert nach Wahrsch.)')\n    ax4.set_title('4. Kumulative Verteilung')\n    ax4.axhline(y=90, color='red', linestyle='--', linewidth=1.5, label='Top-P = 0.9')\n    ax4.axhline(y=50, color='orange', linestyle='--', linewidth=1.5, label='Top-P = 0.5')\n    ax4.legend(fontsize=8)\n    ax4.set_ylim(0, 105)\n    \n    # %-Werte an den Punkten\n    for i, (c, w) in enumerate(zip(cumsum, top_words)):\n        ax4.text(i, c * 100 + 2, f'{c*100:.1f}%', ha='center', fontsize=8)\n    \n    fig.suptitle(f'Top-{top_k} Vorhersagen nach \"{text}\"\\n'\n                 r'Pipeline: Logits $\\rightarrow$ softmax$(x_i) = e^{x_i} \\;/\\; \\Sigma \\, e^{x_j}$ $\\rightarrow$ Wahrscheinlichkeit',\n                 fontsize=13, y=1.04)\n    plt.tight_layout()\n    plt.show()\n    \n    return top_words, top_probs, top_logits\n\n# Visualisieren\nplot_top_k_bar(model, tokenizer, \"die katze\", top_k=12);",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vollständige Logits-Heatmap (alle Wörter)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_full_logits_heatmap(model, tokenizer, text):\n",
    "    \"\"\"Zeigt alle Logits als Heatmap - ein Wert pro Wort im Vokabular.\"\"\"\n",
    "    logits, probs, _ = get_logits_and_probs(model, tokenizer, text)\n",
    "    \n",
    "    # Sortiere nach Wahrscheinlichkeit\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_logits = logits[sorted_indices]\n",
    "    sorted_words = [tokenizer.idx_to_word.get(i, \"?\")[:10] for i in sorted_indices]\n",
    "    \n",
    "    # Heatmap (als 2D-Matrix reshaped)\n",
    "    n_words = len(sorted_logits)\n",
    "    cols = 10\n",
    "    rows = (n_words + cols - 1) // cols\n",
    "    \n",
    "    # Padding falls nötig\n",
    "    padded_logits = np.pad(sorted_logits, (0, rows * cols - n_words), constant_values=np.nan)\n",
    "    matrix = padded_logits.reshape(rows, cols)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, max(6, rows * 0.5)))\n",
    "    \n",
    "    im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, label='Logit-Wert')\n",
    "    \n",
    "    # Labels\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i * cols + j\n",
    "            if idx < n_words:\n",
    "                word = sorted_words[idx][:6]\n",
    "                val = sorted_logits[idx]\n",
    "                color = 'white' if abs(val) > 1 else 'black'\n",
    "                ax.text(j, i, f\"{word}\\n{val:.1f}\", ha='center', va='center', \n",
    "                       fontsize=7, color=color)\n",
    "    \n",
    "    ax.set_title(f'Alle Logits für \"{text}\" (sortiert nach Wahrscheinlichkeit)')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_full_logits_heatmap(model, tokenizer, \"die katze\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generierungsprozess Schritt für Schritt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def visualize_generation_steps(model, tokenizer, start_text, max_steps=6, top_k=8):\n    \"\"\"Visualisiert jeden Schritt der Text-Generierung.\"\"\"\n    model.eval()\n    \n    tokens = tokenizer.encode(start_text)\n    generated_words = start_text.split()\n    \n    # Erst alle Schritte berechnen, dann plotten\n    steps_data = []\n    \n    for step in range(max_steps):\n        context = tokens[-5:] if len(tokens) > 5 else tokens\n        \n        with torch.no_grad():\n            input_tensor = torch.tensor(context).unsqueeze(0)\n            logits = model(input_tensor)\n            last_logits = logits[0, -1, :]\n            probs = F.softmax(last_logits, dim=-1).numpy()\n        \n        top_indices = np.argsort(probs)[-top_k:][::-1]\n        top_probs = probs[top_indices]\n        top_words = [tokenizer.idx_to_word.get(i, \"?\") for i in top_indices]\n        \n        next_token = top_indices[0]\n        next_word = top_words[0]\n        context_text = tokenizer.decode(context)\n        \n        steps_data.append({\n            'top_probs': top_probs,\n            'top_words': top_words,\n            'context_text': context_text,\n            'next_token': next_token,\n        })\n        \n        # EOS Check\n        if next_token == tokenizer.word_to_idx.get(tokenizer.eos_token):\n            break\n        \n        tokens.append(next_token)\n        generated_words.append(next_word)\n    \n    # Alle Schritte nebeneinander – Breite passt sich exakt an Anzahl an\n    n_steps = len(steps_data)\n    fig, axes = plt.subplots(1, n_steps, figsize=(3.5 * n_steps, 5),\n                             squeeze=False)\n    axes = axes[0]  # squeeze=False gibt 2D-Array, erste Zeile nehmen\n    \n    for step, (ax, data) in enumerate(zip(axes, steps_data)):\n        colors = plt.cm.Blues(np.linspace(0.8, 0.3, top_k))\n        bars = ax.barh(range(top_k), data['top_probs'] * 100, color=colors)\n        ax.set_yticks(range(top_k))\n        ax.set_yticklabels(data['top_words'])\n        ax.set_xlabel('Wahrsch. (%)')\n        ax.set_title(f'Schritt {step+1}: \"{data[\"context_text\"]}\" → ?', fontsize=10)\n        ax.invert_yaxis()\n        \n        # Markiere gewähltes Wort\n        bars[0].set_color('lime')\n        \n        if data['next_token'] == tokenizer.word_to_idx.get(tokenizer.eos_token):\n            ax.annotate('EOS!', xy=(0.5, 0.5), xycoords='axes fraction',\n                       fontsize=20, color='red', ha='center')\n    \n    plt.suptitle(f'Generierung: \"{\" \".join(generated_words)}\"', fontsize=14, y=1.02)\n    plt.tight_layout()\n    plt.show()\n    \n    return \" \".join(generated_words)\n\n# Visualisieren\nresult = visualize_generation_steps(model, tokenizer, \"die katze\", max_steps=6)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vergleich mehrerer Eingaben"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_inputs_heatmap(model, tokenizer, texts, top_k=10):\n",
    "    \"\"\"Vergleicht die Top-K Vorhersagen für mehrere Eingaben als Heatmap.\"\"\"\n",
    "    \n",
    "    # Sammle alle Top-Wörter\n",
    "    all_top_words = set()\n",
    "    text_probs = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        _, probs, _ = get_logits_and_probs(model, tokenizer, text)\n",
    "        top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "        top_words = [tokenizer.idx_to_word.get(i, \"?\") for i in top_indices]\n",
    "        all_top_words.update(top_words)\n",
    "        text_probs[text] = {tokenizer.idx_to_word.get(i, \"?\"): probs[i] for i in top_indices}\n",
    "    \n",
    "    # Sortiere Wörter\n",
    "    all_top_words = sorted(all_top_words)\n",
    "    \n",
    "    # Erstelle Matrix\n",
    "    matrix = np.zeros((len(texts), len(all_top_words)))\n",
    "    for i, text in enumerate(texts):\n",
    "        for j, word in enumerate(all_top_words):\n",
    "            matrix[i, j] = text_probs[text].get(word, 0) * 100\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(max(12, len(all_top_words) * 0.8), len(texts) * 1.5 + 2))\n",
    "    \n",
    "    im = ax.imshow(matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, label='Wahrscheinlichkeit (%)')\n",
    "    \n",
    "    ax.set_xticks(range(len(all_top_words)))\n",
    "    ax.set_xticklabels(all_top_words, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(texts)))\n",
    "    ax.set_yticklabels([f'\"{t}\"' for t in texts])\n",
    "    \n",
    "    # Werte in Zellen\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(all_top_words)):\n",
    "            val = matrix[i, j]\n",
    "            if val > 0.5:\n",
    "                color = 'white' if val > 5 else 'black'\n",
    "                ax.text(j, i, f'{val:.1f}', ha='center', va='center', fontsize=8, color=color)\n",
    "    \n",
    "    ax.set_title('Vergleich: Welches Wort kommt nach...?')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vergleiche\n",
    "compare_inputs_heatmap(model, tokenizer, [\n",
    "    \"die katze\",\n",
    "    \"der hund\",\n",
    "    \"das kind\",\n",
    "    \"die sonne\"\n",
    "], top_k=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Temperatur-Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compare_temperatures(model, tokenizer, text, temperatures=[0.0, 0.5, 1.0, 1.5, 2.0], top_k=8):\n    \"\"\"Zeigt wie Temperature die Wahrscheinlichkeitsverteilung beeinflusst.\n    \n    Temperature = 0: Argmax (deterministisch, Top-Wort bekommt 100%)\n    Temperature = 1: Standard Softmax\n    Temperature > 1: Flachere Verteilung (mehr Zufall)\n    Temperature < 1: Schärfere Verteilung (weniger Zufall)\n    \"\"\"\n    logits, _, _ = get_logits_and_probs(model, tokenizer, text)\n    \n    # Top-K basierend auf originalen Logits (gleich für alle Temperaturen!)\n    top_indices = np.argsort(logits)[-top_k:][::-1]\n    top_words = [tokenizer.idx_to_word.get(i, \"?\")[:8] for i in top_indices]\n    top_logits = logits[top_indices]\n    \n    # 1 Logit-Panel + N Temperatur-Panels\n    n_panels = 1 + len(temperatures)\n    fig, axes = plt.subplots(1, n_panels, figsize=(4 * n_panels, 5))\n    \n    # --- 1. Rohe Logits (vor Softmax) ---\n    ax0 = axes[0]\n    colors_logits = ['#2ecc71' if l > 0 else '#e74c3c' for l in top_logits]\n    bars0 = ax0.barh(range(top_k), top_logits, color=colors_logits, alpha=0.8)\n    ax0.set_yticks(range(top_k))\n    ax0.set_yticklabels(top_words)\n    ax0.set_xlabel('Logit-Wert')\n    ax0.set_title('Logits (roh)', fontweight='bold')\n    ax0.axvline(x=0, color='white', linestyle='--', alpha=0.5)\n    ax0.invert_yaxis()\n    for bar, val in zip(bars0, top_logits):\n        x_pos = bar.get_width() + 0.1 if val >= 0 else bar.get_width() - 0.1\n        ha = 'left' if val >= 0 else 'right'\n        ax0.text(x_pos, bar.get_y() + bar.get_height()/2,\n                f'{val:.1f}', va='center', ha=ha, fontsize=9)\n    \n    # --- 2-N. Temperatur-Panels ---\n    for ax, temp in zip(axes[1:], temperatures):\n        if temp == 0:\n            # Temperatur 0 = Argmax: Top-Wort bekommt 100%, Rest 0%\n            probs = np.zeros_like(logits)\n            probs[np.argmax(logits)] = 1.0\n        else:\n            # Logits mit Temperature skalieren\n            scaled_logits = logits / temp\n            probs = F.softmax(torch.tensor(scaled_logits), dim=-1).numpy()\n\n        # Wahrscheinlichkeiten für die (gleichen) Top-K Wörter\n        top_probs = probs[top_indices]\n        \n        # Plot\n        colors = plt.cm.coolwarm(np.linspace(0.8, 0.2, top_k))\n        bars = ax.barh(range(top_k), top_probs * 100, color=colors)\n        ax.set_yticks(range(top_k))\n        ax.set_yticklabels(top_words)\n        ax.set_xlabel('Wahrsch. (%)')\n        ax.set_title(f'Temp = {temp}')\n        ax.invert_yaxis()\n        ax.set_xlim(0, 100)\n        \n        # %-Werte neben den Balken\n        for bar, p in zip(bars, top_probs):\n            ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n                    f'{p*100:.1f}%', va='center', fontsize=8)\n    \n    plt.suptitle(f'Temperatur-Effekt auf \"{text}\"\\n'\n                 r'Logits $\\rightarrow$ Logits / T $\\rightarrow$ Softmax $\\rightarrow$ Wahrscheinlichkeit'\n                 '\\n(0 = argmax, niedriger = fokussierter, höher = zufälliger)', y=1.08)\n    plt.tight_layout()\n    plt.show()\n\ncompare_temperatures(model, tokenizer, \"die katze\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Embedding-Visualisierung\n",
    "\n",
    "Embeddings sind hochdimensionale Vektoren (z.B. 32D), die jedes Wort im Vokabular repräsentieren.\n",
    "Ähnliche Wörter sollten ähnliche Vektoren haben und im Embedding-Raum nahe beieinander liegen.\n",
    "\n",
    "**Visualisierungsmethoden:**\n",
    "- **PCA** (Principal Component Analysis): Schnell, linear, zeigt globale Struktur\n",
    "- **t-SNE**: Langsamer, nicht-linear, zeigt Cluster besser\n",
    "\n",
    "Das Modell lernt diese Vektoren während des Trainings!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef plot_embeddings_pca(model, tokenizer, highlight_words=None, figsize=(12, 10)):\n    \"\"\"\n    Visualisiert die Embedding-Vektoren mit PCA (2D Projektion).\n    \n    Args:\n        model: Das trainierte Modell\n        tokenizer: Der Tokenizer mit dem Vokabular\n        highlight_words: Liste von Wörtern, die hervorgehoben werden sollen\n        figsize: Größe der Figur\n    \"\"\"\n    # Embedding-Gewichte extrahieren\n    embeddings = model.embedding.weight.detach().numpy()\n    vocab_size, embed_dim = embeddings.shape\n    \n    print(f\"Embedding-Matrix: {vocab_size} Wörter × {embed_dim} Dimensionen\")\n    \n    # PCA auf 2D reduzieren\n    pca = PCA(n_components=2)\n    embeddings_2d = pca.fit_transform(embeddings)\n    \n    print(f\"Erklärte Varianz: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n    print(f\"  - PC1: {pca.explained_variance_ratio_[0]*100:.1f}%\")\n    print(f\"  - PC2: {pca.explained_variance_ratio_[1]*100:.1f}%\")\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Alle Wörter als Punkte\n    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n               alpha=0.6, s=50, c='steelblue')\n    \n    # Wörter beschriften\n    for i in range(vocab_size):\n        word = tokenizer.idx_to_word.get(i, f\"[{i}]\")\n        x, y = embeddings_2d[i]\n        \n        # Hervorheben falls in Liste\n        if highlight_words and word in highlight_words:\n            ax.scatter([x], [y], s=200, c='red', marker='*', zorder=5)\n            ax.annotate(word, (x, y), fontsize=12, fontweight='bold',\n                       color='red', ha='center', va='bottom',\n                       xytext=(0, 8), textcoords='offset points')\n        else:\n            ax.annotate(word, (x, y), fontsize=8, alpha=0.7,\n                       ha='center', va='bottom')\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n    ax.set_title('Embedding-Visualisierung (PCA)')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return embeddings_2d, pca\n\n\ndef plot_embeddings_tsne(model, tokenizer, perplexity=5, figsize=(12, 10)):\n    \"\"\"\n    Visualisiert die Embedding-Vektoren mit t-SNE (2D Projektion).\n    t-SNE ist besser für Cluster, aber langsamer und nicht deterministisch.\n    \n    Args:\n        model: Das trainierte Modell\n        tokenizer: Der Tokenizer mit dem Vokabular\n        perplexity: t-SNE Parameter (5-50, kleiner = lokaler Fokus)\n        figsize: Größe der Figur\n    \"\"\"\n    # Embedding-Gewichte extrahieren\n    embeddings = model.embedding.weight.detach().numpy()\n    vocab_size, embed_dim = embeddings.shape\n    \n    print(f\"Embedding-Matrix: {vocab_size} Wörter × {embed_dim} Dimensionen\")\n    print(f\"t-SNE mit perplexity={perplexity}...\")\n    \n    # t-SNE auf 2D reduzieren\n    tsne = TSNE(n_components=2, perplexity=min(perplexity, vocab_size-1), \n                random_state=42, n_iter=1000)\n    embeddings_2d = tsne.fit_transform(embeddings)\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Farben nach Wortart (einfache Heuristik)\n    colors = []\n    for i in range(vocab_size):\n        word = tokenizer.idx_to_word.get(i, \"\")\n        if word.startswith(\"<\"):  # Special tokens\n            colors.append('gray')\n        elif word in ['der', 'die', 'das', 'den', 'dem', 'ein', 'eine']:\n            colors.append('green')  # Artikel\n        elif word in ['auf', 'in', 'im', 'am', 'vom', 'mit', 'über', 'neben']:\n            colors.append('orange')  # Präpositionen\n        else:\n            colors.append('steelblue')  # Rest\n    \n    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n               alpha=0.7, s=80, c=colors)\n    \n    # Wörter beschriften\n    for i in range(vocab_size):\n        word = tokenizer.idx_to_word.get(i, f\"[{i}]\")\n        x, y = embeddings_2d[i]\n        ax.annotate(word, (x, y), fontsize=8, alpha=0.8,\n                   ha='center', va='bottom')\n    \n    ax.set_xlabel('t-SNE Dimension 1')\n    ax.set_ylabel('t-SNE Dimension 2')\n    ax.set_title('Embedding-Visualisierung (t-SNE)')\n    ax.grid(True, alpha=0.3)\n    \n    # Legende\n    from matplotlib.patches import Patch\n    legend_elements = [\n        Patch(facecolor='green', label='Artikel'),\n        Patch(facecolor='orange', label='Präpositionen'),\n        Patch(facecolor='steelblue', label='Andere'),\n        Patch(facecolor='gray', label='Special Tokens')\n    ]\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return embeddings_2d\n\n\ndef plot_embedding_similarity(model, tokenizer, words, figsize=(10, 8)):\n    \"\"\"\n    Zeigt die Kosinus-Ähnlichkeit zwischen ausgewählten Wörtern als Heatmap.\n    \n    Args:\n        model: Das trainierte Modell\n        tokenizer: Der Tokenizer\n        words: Liste von Wörtern zum Vergleichen\n    \"\"\"\n    # Embedding-Vektoren für die Wörter holen\n    embeddings = model.embedding.weight.detach().numpy()\n    \n    word_indices = []\n    valid_words = []\n    for word in words:\n        idx = tokenizer.word_to_idx.get(word.lower())\n        if idx is not None:\n            word_indices.append(idx)\n            valid_words.append(word)\n        else:\n            print(f\"Warnung: '{word}' nicht im Vokabular\")\n    \n    if len(valid_words) < 2:\n        print(\"Mindestens 2 gültige Wörter benötigt!\")\n        return\n    \n    # Embedding-Vektoren extrahieren\n    word_embeddings = embeddings[word_indices]\n    \n    # Kosinus-Ähnlichkeit berechnen\n    # cos_sim(a, b) = (a · b) / (||a|| * ||b||)\n    norms = np.linalg.norm(word_embeddings, axis=1, keepdims=True)\n    normalized = word_embeddings / norms\n    similarity_matrix = normalized @ normalized.T\n    \n    # Heatmap\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    im = ax.imshow(similarity_matrix, cmap='RdYlGn', vmin=-1, vmax=1)\n    plt.colorbar(im, ax=ax, label='Kosinus-Ähnlichkeit')\n    \n    ax.set_xticks(range(len(valid_words)))\n    ax.set_yticks(range(len(valid_words)))\n    ax.set_xticklabels(valid_words, rotation=45, ha='right')\n    ax.set_yticklabels(valid_words)\n    \n    # Werte in Zellen\n    for i in range(len(valid_words)):\n        for j in range(len(valid_words)):\n            val = similarity_matrix[i, j]\n            color = 'white' if abs(val) > 0.5 else 'black'\n            ax.text(j, i, f'{val:.2f}', ha='center', va='center', \n                   fontsize=10, color=color)\n    \n    ax.set_title('Embedding-Ähnlichkeit (Kosinus)')\n    plt.tight_layout()\n    plt.show()\n    \n    return similarity_matrix, valid_words\n\n\nprint(\"Embedding-Visualisierungsfunktionen geladen!\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# PCA Visualisierung - alle Wörter im 2D-Raum\n",
    "# Ähnliche Wörter sollten nahe beieinander liegen\n",
    "plot_embeddings_pca(model, tokenizer, highlight_words=['katze', 'hund', 'die', 'der', 'kinder'])"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# t-SNE Visualisierung - zeigt Cluster besser\n# Farben: Grün=Artikel, Orange=Präpositionen, Blau=Rest\nplot_embeddings_tsne(model, tokenizer, perplexity=8)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Ähnlichkeits-Heatmap für ausgewählte Wörter\n# Wert 1.0 = identisch, 0 = unkorreliert, -1 = entgegengesetzt\nplot_embedding_similarity(model, tokenizer, [\n    'katze', 'hund', 'kind',      # Subjekte\n    'sitzt', 'läuft', 'spielt',   # Verben\n    'die', 'der', 'das',          # Artikel\n    'auf', 'im', 'am'             # Präpositionen\n])",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
