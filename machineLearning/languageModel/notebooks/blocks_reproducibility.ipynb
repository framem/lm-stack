{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Blocks & Wiederholbarkeit\n",
        "\n",
        "Dieses Notebook erklaert:\n",
        "1. **Inference-Pipeline**: Token → Embedding → Transformer Blocks → Logits → Softmax → Token\n",
        "2. **Transformer Blocks**: Was passiert in jedem Block und warum braucht man mehrere?\n",
        "3. **Wiederholbarkeit**: Warum liefert dasselbe Modell mit demselben Prompt unterschiedliche Ergebnisse?\n",
        "4. **Floating-Point-Arithmetik**: Warum `(a + b) + c != a + (b + c)` auf dem Computer\n",
        "5. **GPU-Parallelismus & Tiling**: Wie `num_ctx` die Berechnung beeinflusst\n",
        "6. **Determinismus-Rezept**: `temperature=0` + `seed` + fester `num_ctx`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "src_path = Path(\"../src\").resolve()\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
        "\n",
        "from training.training_transformer import load_transformer_model\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams.update({'axes.grid': True, 'grid.alpha': 0.3})\n",
        "\n",
        "print(\"Bibliotheken geladen!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_dir = Path(\"../dist/transformer_model\")\n",
        "\n",
        "if not model_dir.exists():\n",
        "    raise FileNotFoundError(f\"Modell nicht gefunden: {model_dir}\\nBitte erst trainieren: python src/main.py\")\n",
        "\n",
        "model, tokenizer = load_transformer_model(str(model_dir))\n",
        "model.eval()\n",
        "\n",
        "print(f\"Modell:     MiniGPT\")\n",
        "print(f\"Vokabular:  {tokenizer.vocab_size} Tokens\")\n",
        "print(f\"Embed-Dim:  {model.embed_dim}\")\n",
        "print(f\"Blocks:     {len(model.blocks)}\")\n",
        "print(f\"Heads:      {model.blocks[0].attention.num_heads}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Die Inference-Pipeline\n",
        "\n",
        "Bei **jeder** Token-Generierung wird das komplette Netz einmal durchlaufen.\n",
        "Logits werden **nicht** gespeichert – sie werden jedes Mal frisch berechnet.\n",
        "\n",
        "```\n",
        "Input: \"die katze\"                 (String)\n",
        "        ↓\n",
        "   [Tokenisierung]                  String → Token-IDs\n",
        "        ↓\n",
        "   Token-IDs: [12, 5]               (Integer-Array)\n",
        "        ↓\n",
        "   [Embedding-Lookup]               Token-ID → Vektor (gespeicherte Gewichte)\n",
        "        ↓\n",
        "   [Positional Encoding]            Position im Satz eincodieren\n",
        "        ↓\n",
        "   [Transformer Block 1]            Attention + FeedForward (Gewichte)\n",
        "        ↓\n",
        "   [Transformer Block 2]            Attention + FeedForward (Gewichte)\n",
        "        ↓\n",
        "   [Transformer Block N ...]        je mehr Blocks, desto \"schlauer\"\n",
        "        ↓\n",
        "   [LayerNorm]\n",
        "        ↓\n",
        "   [Projektion auf Vokabular]       Matrix: embed_dim × vocab_size\n",
        "        ↓\n",
        "   Logits: [2.1, -0.5, 8.7, ...]   (ein Score pro Token im Vokabular)\n",
        "        ↓\n",
        "   [Softmax + Sampling]             Logits → Wahrscheinlichkeiten → Auswahl\n",
        "        ↓\n",
        "   Output-Token: \"sitzt\"            (ein neues Token)\n",
        "```\n",
        "\n",
        "**Gespeichert** (Gewichte, ~8 GB bei einem 8B-Modell): Embedding-Matrix, Attention-Gewichte (Q, K, V), Feed-Forward-Gewichte\n",
        "\n",
        "**Berechnet** (pro Token, fluechtiger): Zwischenaktivierungen, KV-Cache, Logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_inference_pipeline(model, tokenizer, text):\n",
        "    \"\"\"Zeigt die komplette Inference-Pipeline mit Zwischenergebnissen.\"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(text)\n",
        "    words = text.lower().split()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "\n",
        "        # 1. Embedding\n",
        "        embedded = model.token_embedding(input_tensor)\n",
        "        embed_norms = embedded[0].norm(dim=-1).numpy()\n",
        "\n",
        "        # 2. Positional Encoding\n",
        "        pos_encoded = embedded + model.pos_embedding[:, :len(tokens), :]\n",
        "\n",
        "        # 3. Durch jeden Block\n",
        "        block_outputs = []\n",
        "        x = pos_encoded\n",
        "        for block in model.blocks:\n",
        "            x = block(x)\n",
        "            block_outputs.append(x[0, -1, :].numpy().copy())\n",
        "\n",
        "        # 4. LayerNorm + Projektion\n",
        "        x = model.ln_final(x)\n",
        "        logits = model.lm_head(x)\n",
        "        last_logits = logits[0, -1, :].numpy()\n",
        "        probs = F.softmax(torch.tensor(last_logits), dim=-1).numpy()\n",
        "\n",
        "    # --- Visualisierung ---\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "    # 1. Embedding-Normen pro Token\n",
        "    ax1 = axes[0, 0]\n",
        "    bars = ax1.bar(range(len(words)), embed_norms, color='#3498db')\n",
        "    ax1.set_xticks(range(len(words)))\n",
        "    ax1.set_xticklabels(words, fontsize=11)\n",
        "    ax1.set_ylabel('Vektor-Norm')\n",
        "    ax1.set_title('1. Embedding: Token → Vektor', fontweight='bold')\n",
        "    for bar, norm in zip(bars, embed_norms):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                f'{norm:.2f}', ha='center', fontsize=10)\n",
        "\n",
        "    # 2. Block-Outputs: Wie veraendert sich der Vektor?\n",
        "    ax2 = axes[0, 1]\n",
        "    block_norms = [np.linalg.norm(bo) for bo in block_outputs]\n",
        "    colors_block = plt.cm.Oranges(np.linspace(0.4, 0.9, len(block_outputs)))\n",
        "    bars2 = ax2.bar(range(len(block_outputs)), block_norms, color=colors_block)\n",
        "    ax2.set_xticks(range(len(block_outputs)))\n",
        "    ax2.set_xticklabels([f'Block {i+1}' for i in range(len(block_outputs))])\n",
        "    ax2.set_ylabel('Vektor-Norm (letzter Token)')\n",
        "    ax2.set_title('2. Transformer Blocks: schrittweise Transformation', fontweight='bold')\n",
        "    for bar, norm in zip(bars2, block_norms):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                f'{norm:.1f}', ha='center', fontsize=10)\n",
        "\n",
        "    # 3. Logits (Top-10)\n",
        "    ax3 = axes[1, 0]\n",
        "    top_k = 10\n",
        "    top_idx = np.argsort(last_logits)[-top_k:][::-1]\n",
        "    top_logits = last_logits[top_idx]\n",
        "    top_words = [tokenizer.idx_to_word.get(i, '?') for i in top_idx]\n",
        "    colors_l = ['#2ecc71' if l > 0 else '#e74c3c' for l in top_logits]\n",
        "    ax3.barh(range(top_k), top_logits, color=colors_l)\n",
        "    ax3.set_yticks(range(top_k))\n",
        "    ax3.set_yticklabels(top_words)\n",
        "    ax3.set_xlabel('Logit-Wert')\n",
        "    ax3.set_title(f'3. Logits: {tokenizer.vocab_size} Scores (Top-{top_k})', fontweight='bold')\n",
        "    ax3.invert_yaxis()\n",
        "    ax3.axvline(x=0, color='white', linestyle='--', alpha=0.3)\n",
        "\n",
        "    # 4. Wahrscheinlichkeiten nach Softmax\n",
        "    ax4 = axes[1, 1]\n",
        "    top_probs = probs[top_idx]\n",
        "    colors_p = plt.cm.viridis(np.linspace(0.8, 0.2, top_k))\n",
        "    bars4 = ax4.barh(range(top_k), top_probs * 100, color=colors_p)\n",
        "    ax4.set_yticks(range(top_k))\n",
        "    ax4.set_yticklabels(top_words)\n",
        "    ax4.set_xlabel('Wahrscheinlichkeit (%)')\n",
        "    ax4.set_title('4. Softmax: Logits → Wahrscheinlichkeiten', fontweight='bold')\n",
        "    ax4.invert_yaxis()\n",
        "    for bar, p in zip(bars4, top_probs):\n",
        "        ax4.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
        "                f'{p*100:.1f}%', va='center', fontsize=9)\n",
        "\n",
        "    plt.suptitle(f'Inference-Pipeline: \"{text}\" → ?', fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_inference_pipeline(model, tokenizer, \"die katze\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Transformer Blocks: Warum braucht man mehrere?\n",
        "\n",
        "Die **letzte Schicht** (Projektion auf Vokabular) ist nur eine lineare Transformation:\n",
        "`Logits = Gewichtsmatrix × Eingabevektor` – sie kann keine Zusammenhaenge erkennen.\n",
        "\n",
        "Die **Transformer Blocks** transformieren den Input schrittweise in etwas Bedeutungsvolles:\n",
        "\n",
        "| Block-Bereich | Was passiert |\n",
        "|---|---|\n",
        "| Fruehe Blocks (1-4) | Syntaktische Muster: \"Das ist ein Wort, das ist ein anderes\" |\n",
        "| Mittlere Blocks (5-20) | Semantische Zusammenhaenge: \"Hauptstadt\" + \"Frankreich\" gehoeren zusammen |\n",
        "| Spaete Blocks (21-32) | Entscheidung: \"Der naechste Token sollte 'Paris' sein\" |\n",
        "\n",
        "Jeder Block besteht aus:\n",
        "1. **Multi-Head Attention**: Welche Tokens im Kontext sind fuer den naechsten relevant?\n",
        "2. **LayerNorm + Residual**: Stabilisierung\n",
        "3. **Feed-Forward (GELU)**: Nicht-lineare Transformation\n",
        "4. **LayerNorm + Residual**: Stabilisierung\n",
        "\n",
        "Unser MiniGPT hat nur 2 Blocks – grosse Modelle (GPT-4, Llama 3) haben 32-96 Blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_block_internals(model, tokenizer, text):\n",
        "    \"\"\"Zeigt was in jedem Transformer Block passiert – mit Attention-Weights.\"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(text)\n",
        "    words = text.lower().split()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "    attention_weights = model.get_attention_weights()\n",
        "    num_blocks = len(attention_weights)\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_blocks + 1, figsize=(6 * (num_blocks + 1), 5))\n",
        "\n",
        "    # Attention-Heatmaps pro Block\n",
        "    for i, layer_weights in enumerate(attention_weights):\n",
        "        ax = axes[i]\n",
        "        avg_weights = layer_weights[0].mean(dim=0).numpy()\n",
        "        im = ax.imshow(avg_weights, cmap='Blues', vmin=0, vmax=1)\n",
        "        ax.set_xticks(range(len(words)))\n",
        "        ax.set_yticks(range(len(words)))\n",
        "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
        "        ax.set_yticklabels(words)\n",
        "        ax.set_xlabel('Key (worauf wird geschaut)')\n",
        "        ax.set_ylabel('Query (wer schaut)')\n",
        "        ax.set_title(f'Block {i+1}: Self-Attention', fontweight='bold')\n",
        "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "\n",
        "    # Architektur-Diagramm\n",
        "    ax_arch = axes[-1]\n",
        "    ax_arch.set_xlim(0, 10)\n",
        "    ax_arch.set_ylim(0, 12)\n",
        "    ax_arch.axis('off')\n",
        "    ax_arch.set_title('Block-Architektur', fontweight='bold')\n",
        "\n",
        "    components = [\n",
        "        (10.5, '#2c3e50', 'Input Tokens'),\n",
        "        (9.0, '#8e44ad', 'Embedding + Position'),\n",
        "        (7.5, '#e67e22', 'Self-Attention'),\n",
        "        (6.0, '#27ae60', 'LayerNorm + Residual'),\n",
        "        (4.5, '#e67e22', 'Feed-Forward (GELU)'),\n",
        "        (3.0, '#27ae60', 'LayerNorm + Residual'),\n",
        "        (1.5, '#c0392b', f'Logits ({tokenizer.vocab_size} Scores)'),\n",
        "    ]\n",
        "\n",
        "    for y, color, label in components:\n",
        "        rect = FancyBboxPatch((1, y - 0.5), 8, 0.9, boxstyle='round,pad=0.1',\n",
        "                             facecolor=color, edgecolor='white', alpha=0.8)\n",
        "        ax_arch.add_patch(rect)\n",
        "        ax_arch.text(5, y, label, ha='center', va='center',\n",
        "                    fontsize=10, color='white', fontweight='bold')\n",
        "\n",
        "    # Pfeile\n",
        "    for i in range(len(components) - 1):\n",
        "        y_from = components[i][0] - 0.5\n",
        "        y_to = components[i+1][0] + 0.5\n",
        "        ax_arch.annotate('', xy=(5, y_to), xytext=(5, y_from),\n",
        "                        arrowprops=dict(arrowstyle='->', color='white', lw=1.5))\n",
        "\n",
        "    # Klammer fuer \"Block\"\n",
        "    ax_arch.annotate('', xy=(9.5, 7.5 + 0.4), xytext=(9.5, 3.0 - 0.4),\n",
        "                    arrowprops=dict(arrowstyle='-', color='yellow', lw=2))\n",
        "    ax_arch.text(9.8, 5.25, f'Block\\n(x{num_blocks})', fontsize=9,\n",
        "                color='yellow', va='center', fontweight='bold')\n",
        "\n",
        "    plt.suptitle(f'Transformer Blocks fuer \"{text}\"', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_block_internals(model, tokenizer, \"die katze sitzt auf\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ohne Transformer Blocks: Nur Embedding → Projektion\n",
        "\n",
        "Was passiert, wenn wir die Blocks ueberspringen und direkt vom Embedding auf das Vokabular projizieren?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_with_without_blocks(model, tokenizer, text, top_k=8):\n",
        "    \"\"\"Vergleicht Vorhersagen mit vs. ohne Transformer Blocks.\"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "\n",
        "        # MIT Blocks (normal)\n",
        "        logits_full = model(input_tensor)\n",
        "        probs_full = F.softmax(logits_full[0, -1, :], dim=-1).numpy()\n",
        "\n",
        "        # OHNE Blocks (Embedding direkt → Projektion)\n",
        "        embedded = model.token_embedding(input_tensor)\n",
        "        embedded = embedded + model.pos_embedding[:, :len(tokens), :]\n",
        "        logits_skip = model.lm_head(model.ln_final(embedded))\n",
        "        probs_skip = F.softmax(logits_skip[0, -1, :], dim=-1).numpy()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    for ax, probs, title, color in [\n",
        "        (ax1, probs_skip, 'OHNE Blocks\\n(Embedding → Projektion)', '#e74c3c'),\n",
        "        (ax2, probs_full, f'MIT {len(model.blocks)} Blocks\\n(vollstaendige Pipeline)', '#2ecc71'),\n",
        "    ]:\n",
        "        top_idx = np.argsort(probs)[-top_k:][::-1]\n",
        "        top_p = probs[top_idx]\n",
        "        top_w = [tokenizer.idx_to_word.get(i, '?') for i in top_idx]\n",
        "\n",
        "        # Entropie berechnen\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "\n",
        "        bars = ax.barh(range(top_k), top_p * 100, color=color, alpha=0.8)\n",
        "        ax.set_yticks(range(top_k))\n",
        "        ax.set_yticklabels(top_w, fontsize=11)\n",
        "        ax.set_xlabel('Wahrscheinlichkeit (%)')\n",
        "        ax.set_title(f'{title}\\nEntropie: {entropy:.2f}', fontweight='bold')\n",
        "        ax.invert_yaxis()\n",
        "        for bar, p in zip(bars, top_p):\n",
        "            ax.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
        "                    f'{p*100:.1f}%', va='center', fontsize=9)\n",
        "\n",
        "    plt.suptitle(f'Warum Transformer Blocks noetig sind: \"{text}\" → ?\\n'\n",
        "                 f'Ohne Blocks: Modell rät zufaellig. Mit Blocks: Modell versteht den Kontext.',\n",
        "                 fontsize=13, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_with_without_blocks(model, tokenizer, \"die katze\")\n",
        "compare_with_without_blocks(model, tokenizer, \"das kind spielt\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Wiederholbarkeit: Das Determinismus-Problem\n",
        "\n",
        "Fuer **deterministische** LLM-Ausgaben muessen drei Parameter zusammenspielen:\n",
        "\n",
        "| Parameter | Wert | Warum |\n",
        "|---|---|---|\n",
        "| `temperature` | `0` | Entfernt Zufaelligkeit – immer das wahrscheinlichste Token (Greedy Decoding) |\n",
        "| `seed` | fester Integer, z.B. `123` | Fixiert den Zufallsgenerator |\n",
        "| `num_ctx` | fester Wert, z.B. `2048` | Identisches Speicher-Layout → identische Float-Berechnungen |\n",
        "\n",
        "### Warum reicht `temperature=0` allein nicht?\n",
        "\n",
        "Selbst bei Greedy Decoding (immer das Top-Token) koennen minimale Floating-Point-Unterschiede\n",
        "die Rangfolge zweier fast gleichwertiger Tokens kippen:\n",
        "\n",
        "```\n",
        "Run 1:  token=\"blau\"  logprob=-1.3841   ← Gewinner\n",
        "        token=\"klar\"  logprob=-1.3842\n",
        "\n",
        "Run 2:  token=\"klar\"  logprob=-1.3841   ← Gewinner (!)  \n",
        "        token=\"blau\"  logprob=-1.3842\n",
        "```\n",
        "\n",
        "Ab diesem einen abweichenden Token divergiert die **gesamte** restliche Generierung,\n",
        "weil jedes folgende Token auf einem anderen Kontext aufbaut."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_temperature_determinism(model, tokenizer, text, num_runs=5):\n",
        "    \"\"\"Demonstriert: temperature=0 → deterministisch, temperature>0 → variabel.\"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    results = {'greedy': [], 'sampled': []}\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        with torch.no_grad():\n",
        "            input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "            logits = model(input_tensor)[0, -1, :]\n",
        "\n",
        "            # Greedy (temp=0): immer Top-Token\n",
        "            greedy_token = torch.argmax(logits).item()\n",
        "            results['greedy'].append(tokenizer.idx_to_word.get(greedy_token, '?'))\n",
        "\n",
        "            # Sampling (temp=0.8): zufaellig gewichtet\n",
        "            scaled = logits / 0.8\n",
        "            probs = F.softmax(scaled, dim=-1)\n",
        "            sampled_token = torch.multinomial(probs, 1).item()\n",
        "            results['sampled'].append(tokenizer.idx_to_word.get(sampled_token, '?'))\n",
        "\n",
        "    print(f'Eingabe: \"{text}\"')\n",
        "    print(f'  temp=0 (greedy):  {results[\"greedy\"]}  → alle gleich? {len(set(results[\"greedy\"])) == 1}')\n",
        "    print(f'  temp=0.8 (sample): {results[\"sampled\"]}  → alle gleich? {len(set(results[\"sampled\"])) == 1}')\n",
        "    print()\n",
        "\n",
        "demonstrate_temperature_determinism(model, tokenizer, \"die katze\", num_runs=8)\n",
        "demonstrate_temperature_determinism(model, tokenizer, \"der hund\", num_runs=8)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def show_logit_gaps(model, tokenizer, texts, top_k=5):\n",
        "    \"\"\"Zeigt den Logit-Abstand zwischen Top-1 und Top-2 Token.\n",
        "    \n",
        "    Kleiner Gap = instabil (anfaellig fuer Nichtdeterminismus)\n",
        "    Grosser Gap = stabil (deterministisch auch bei Float-Schwankungen)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(texts), figsize=(6 * len(texts), 5))\n",
        "    if len(texts) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, text in zip(axes, texts):\n",
        "        tokens = tokenizer.encode(text)\n",
        "        with torch.no_grad():\n",
        "            logits = model(torch.tensor(tokens).unsqueeze(0))[0, -1, :].numpy()\n",
        "\n",
        "        top_idx = np.argsort(logits)[-top_k:][::-1]\n",
        "        top_logits = logits[top_idx]\n",
        "        top_words = [tokenizer.idx_to_word.get(i, '?') for i in top_idx]\n",
        "\n",
        "        # Gap zwischen #1 und #2\n",
        "        gap = top_logits[0] - top_logits[1]\n",
        "        stability = 'STABIL' if gap > 0.5 else 'INSTABIL' if gap < 0.1 else 'GRENZWERTIG'\n",
        "        stability_color = '#2ecc71' if gap > 0.5 else '#e74c3c' if gap < 0.1 else '#f39c12'\n",
        "\n",
        "        colors = [stability_color if i == 0 else '#3498db' for i in range(top_k)]\n",
        "        bars = ax.barh(range(top_k), top_logits, color=colors)\n",
        "        ax.set_yticks(range(top_k))\n",
        "        ax.set_yticklabels(top_words, fontsize=11)\n",
        "        ax.set_xlabel('Logit-Wert')\n",
        "        ax.set_title(f'\"{text}\" → ?\\nGap: {gap:.4f} → {stability}',\n",
        "                    fontweight='bold', color=stability_color)\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        # Gap-Annotation\n",
        "        ax.annotate('', xy=(top_logits[1], 0.5), xytext=(top_logits[0], 0.5),\n",
        "                   arrowprops=dict(arrowstyle='<->', color='yellow', lw=2))\n",
        "        ax.text((top_logits[0] + top_logits[1]) / 2, 0.5,\n",
        "               f'Gap: {gap:.4f}', ha='center', va='bottom',\n",
        "               fontsize=9, color='yellow', fontweight='bold')\n",
        "\n",
        "    plt.suptitle('Logit-Gap Analyse: Wie stabil ist die Token-Wahl?\\n'\n",
        "                 'Kleiner Gap = anfaellig fuer Float-Schwankungen → Nichtdeterminismus',\n",
        "                 fontsize=13, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_logit_gaps(model, tokenizer, [\"die katze\", \"der hund\", \"das kind spielt\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Floating-Point-Arithmetik: Warum `(a + b) + c ≠ a + (b + c)`\n",
        "\n",
        "Computer rechnen mit endlicher Praezision (float32 ≈ 7 Dezimalstellen).\n",
        "Dadurch ist **Addition nicht assoziativ** – die Reihenfolge der Operationen beeinflusst das Ergebnis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import struct\n",
        "\n",
        "def float_to_bits(f):\n",
        "    \"\"\"Zeigt die Binaerdarstellung eines float32.\"\"\"\n",
        "    return format(struct.unpack('!I', struct.pack('!f', f))[0], '032b')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DEMO: Floating-Point ist nicht assoziativ\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Beispiel 1: Einfache Addition\n",
        "a = np.float32(1.0)\n",
        "b = np.float32(1e-7)\n",
        "c = np.float32(1e-7)\n",
        "\n",
        "result1 = np.float32(np.float32(a + b) + c)  # (a + b) + c\n",
        "result2 = np.float32(a + np.float32(b + c))   # a + (b + c)\n",
        "\n",
        "print(f\"\\na = {a}, b = {b}, c = {c}\")\n",
        "print(f\"(a + b) + c = {result1:.15f}\")\n",
        "print(f\"a + (b + c) = {result2:.15f}\")\n",
        "print(f\"Differenz:    {abs(result1 - result2):.2e}\")\n",
        "print(f\"Gleich?       {result1 == result2}\")\n",
        "\n",
        "# Beispiel 2: Groessere Summe (wie in einer Attention-Berechnung)\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(\"DEMO: Unterschiedliche Summationsreihenfolge\")\n",
        "print(\"(simuliert GPU-Reduktionsbaum mit verschiedenen Tile-Groessen)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "np.random.seed(42)\n",
        "values = np.random.randn(1024).astype(np.float32)  # 1024 \"Attention Scores\"\n",
        "\n",
        "# Sequentielle Summe (wie CPU)\n",
        "seq_sum = np.float32(0.0)\n",
        "for v in values:\n",
        "    seq_sum = np.float32(seq_sum + v)\n",
        "\n",
        "# Reduktionsbaum mit Blockgroesse 64 (wie Flash Attention mit num_ctx=1024)\n",
        "def tree_reduce(arr, block_size):\n",
        "    \"\"\"Simuliert GPU-Reduktion: erst Bloecke summieren, dann Bloecke zusammenfuehren.\"\"\"\n",
        "    blocks = [arr[i:i+block_size] for i in range(0, len(arr), block_size)]\n",
        "    block_sums = [np.float32(sum(b.astype(np.float32))) for b in blocks]\n",
        "    # Paarweise zusammenfuehren (Reduktionsbaum)\n",
        "    while len(block_sums) > 1:\n",
        "        new_sums = []\n",
        "        for i in range(0, len(block_sums), 2):\n",
        "            if i + 1 < len(block_sums):\n",
        "                new_sums.append(np.float32(block_sums[i] + block_sums[i+1]))\n",
        "            else:\n",
        "                new_sums.append(block_sums[i])\n",
        "        block_sums = new_sums\n",
        "    return block_sums[0]\n",
        "\n",
        "# Verschiedene Blockgroessen simulieren verschiedene num_ctx-Werte\n",
        "block_sizes = [32, 64, 128, 256, 512]\n",
        "print(f\"\\nSequentielle Summe:  {seq_sum:.10f}\")\n",
        "print()\n",
        "for bs in block_sizes:\n",
        "    result = tree_reduce(values, bs)\n",
        "    diff = abs(result - seq_sum)\n",
        "    print(f\"Blockgroesse {bs:>3}: {result:.10f}  (Diff: {diff:.2e})\")\n",
        "\n",
        "print(f\"\\n→ Alle Ergebnisse SOLLTEN gleich sein, sind es aber nicht!\")\n",
        "print(f\"  Diese winzigen Unterschiede koennen bei knappen Logit-Entscheidungen\")\n",
        "print(f\"  die Token-Wahl kippen.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. GPU-Parallelismus: Reduktionsbaeume & Tiling\n",
        "\n",
        "Die GPU rechnet **nicht** sequentiell `a + b + c + d`, sondern parallel in einem **Reduktionsbaum**:\n",
        "\n",
        "```\n",
        "CPU (sequentiell):   ((a + b) + c) + d\n",
        "\n",
        "GPU (parallel):      Thread 1: temp1 = a + b  |  Thread 2: temp2 = c + d\n",
        "                                  ↘                  ↙\n",
        "                            result = temp1 + temp2\n",
        "```\n",
        "\n",
        "Das ist `(a + b) + (c + d)` statt `((a + b) + c) + d` – mathematisch identisch,\n",
        "bei Floats jedoch nicht!\n",
        "\n",
        "### Wie `num_ctx` die Block-Aufteilung beeinflusst\n",
        "\n",
        "Flash Attention teilt die Berechnung in **Tiles** (typisch 64-256 Tokens) auf.\n",
        "Die Tile-Groesse haengt von `num_ctx` ab:\n",
        "\n",
        "```\n",
        "num_ctx = 1024, Tile-Groesse = 128\n",
        "→ 1024 / 128 = 8 Tiles (sauber)\n",
        "\n",
        "num_ctx = 1023, Tile-Groesse = 128  \n",
        "→ 1023 / 128 = 7 Tiles + 1 Rest-Tile mit 127 Tokens (Padding noetig!)\n",
        "```\n",
        "\n",
        "Der Reduktionsbaum sieht dann so aus:\n",
        "\n",
        "```\n",
        "Tile:     [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n",
        "            ↘   ↙       ↘   ↙       ↘   ↙       ↘   ↙\n",
        "Ebene 1:   [1+2]       [3+4]       [5+6]       [7+8]\n",
        "               ↘       ↙               ↘       ↙\n",
        "Ebene 2:       [1-4]                   [5-8]\n",
        "                   ↘               ↙\n",
        "Ebene 3:               [1-8]\n",
        "```\n",
        "\n",
        "Anderes `num_ctx` → anderer Baum → andere Float-Rundung → andere Logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_reduction_tree(n_tiles, ax, title):\n",
        "    \"\"\"Zeichnet einen Reduktionsbaum fuer n_tiles Eingabe-Bloecke.\"\"\"\n",
        "    ax.set_xlim(-1, n_tiles)\n",
        "\n",
        "    levels = []\n",
        "    current = list(range(1, n_tiles + 1))\n",
        "    levels.append(current)\n",
        "    while len(current) > 1:\n",
        "        next_level = []\n",
        "        for i in range(0, len(current), 2):\n",
        "            if i + 1 < len(current):\n",
        "                next_level.append(f'{current[i]}-{current[i+1]}' if isinstance(current[i], int)\n",
        "                                 else f'{current[i].split(\"-\")[0]}-{current[i+1].split(\"-\")[-1]}')\n",
        "            else:\n",
        "                next_level.append(current[i])  # Ungerade Anzahl: Rest\n",
        "        current = next_level\n",
        "        levels.append(current)\n",
        "\n",
        "    total_levels = len(levels)\n",
        "    ax.set_ylim(-0.5, total_levels - 0.5)\n",
        "\n",
        "    # Nodes und Kanten zeichnen\n",
        "    node_positions = {}  # (level, idx) -> x\n",
        "    for level_idx, level in enumerate(levels):\n",
        "        y = total_levels - 1 - level_idx\n",
        "        n = len(level)\n",
        "        spacing = (n_tiles - 1) / max(n - 1, 1) if n > 1 else 0\n",
        "        start_x = (n_tiles - 1) / 2 - spacing * (n - 1) / 2\n",
        "\n",
        "        for i, label in enumerate(level):\n",
        "            x = start_x + i * spacing\n",
        "            node_positions[(level_idx, i)] = x\n",
        "\n",
        "            color = '#3498db' if level_idx == 0 else '#e67e22' if level_idx < total_levels - 1 else '#2ecc71'\n",
        "            ax.scatter(x, y, s=400, color=color, zorder=5, edgecolors='white', linewidth=1)\n",
        "            ax.text(x, y, str(label), ha='center', va='center',\n",
        "                   fontsize=7 if level_idx == 0 else 6, color='white', fontweight='bold')\n",
        "\n",
        "            # Kanten zu Kindern\n",
        "            if level_idx > 0:\n",
        "                parent_idx = level_idx - 1\n",
        "                child_base = i * 2\n",
        "                for c in [child_base, child_base + 1]:\n",
        "                    if (parent_idx, c) in node_positions:\n",
        "                        cx = node_positions[(parent_idx, c)]\n",
        "                        cy = total_levels - 1 - parent_idx\n",
        "                        ax.plot([cx, x], [cy, y], color='white', alpha=0.5, lw=1)\n",
        "\n",
        "    ax.set_title(title, fontweight='bold', fontsize=11)\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "visualize_reduction_tree(8, axes[0], 'num_ctx=1024\\n8 Tiles × 128 Tokens\\n(saubere Aufteilung)')\n",
        "visualize_reduction_tree(9, axes[1], 'num_ctx=1023\\n7 Tiles + 1 Rest-Tile\\n(Padding noetig)')\n",
        "visualize_reduction_tree(16, axes[2], 'num_ctx=2048\\n16 Tiles × 128 Tokens\\n(anderer Baum!)')\n",
        "\n",
        "plt.suptitle('GPU-Reduktionsbaeume: Unterschiedliches num_ctx → unterschiedliche Baumstruktur\\n'\n",
        "             '→ unterschiedliche Float-Rundung → potenziell unterschiedliche Logits',\n",
        "             fontsize=13, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simulate_tiling_effect():\n",
        "    \"\"\"Simuliert den Effekt verschiedener Tile-Groessen auf die Attention-Berechnung.\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Simulierte Attention-Scores (wie sie in Flash Attention vorkommen)\n",
        "    context_sizes = [512, 1024, 1023, 2048, 4096]\n",
        "    tile_size = 128\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Simulation: Wie Tile-Groesse die Berechnung beeinflusst\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    base_scores = np.random.randn(4096).astype(np.float32)\n",
        "\n",
        "    results = {}\n",
        "    for ctx in context_sizes:\n",
        "        scores = base_scores[:ctx]\n",
        "        n_tiles = (ctx + tile_size - 1) // tile_size\n",
        "        has_rest = ctx % tile_size != 0\n",
        "        rest_size = ctx % tile_size if has_rest else tile_size\n",
        "\n",
        "        result = tree_reduce(scores, tile_size)\n",
        "        results[ctx] = result\n",
        "\n",
        "        rest_info = f\" + Rest-Tile ({rest_size})\" if has_rest else \" (sauber)\"\n",
        "        print(f\"  num_ctx={ctx:>4}: {n_tiles:>2} Tiles{rest_info:<20}  Summe = {result:.10f}\")\n",
        "\n",
        "    # Vergleich: 1024 vs 1023\n",
        "    if 1024 in results and 1023 in results:\n",
        "        diff = abs(results[1024] - results[1023])\n",
        "        print(f\"\\n  Differenz 1024 vs 1023: {diff:.2e}\")\n",
        "        print(f\"  → Nur 1 Token Unterschied im Context, aber anderes Ergebnis!\")\n",
        "\n",
        "simulate_tiling_effect()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Der Schmetterlingseffekt: Ein Token kippt alles\n",
        "\n",
        "Autoregressive Generierung ist eine **Kette**: Jedes Token wird basierend auf **allen bisherigen** Tokens gewaehlt.\n",
        "Wenn ein einziges Token abweicht, divergiert der gesamte restliche Text.\n",
        "\n",
        "Reales Beispiel aus einem Ollama-Test mit `qwen3:8b`:\n",
        "\n",
        "```\n",
        "Run 1: \"...discovers a mysterious world of digital reality and rebels...\"\n",
        "Run 2: \"...discovers the world is a computer simulation and joins a rebellion...\"\n",
        "```\n",
        "\n",
        "Die Divergenz passiert nach `discovers` – einmal kommt `a`, einmal `the`.\n",
        "Ab da sind es komplett unterschiedliche Pfade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_butterfly_effect(model, tokenizer, text, max_steps=6):\n",
        "    \"\"\"Zeigt wie verschiedene Sampling-Entscheidungen zu divergierenden Pfaden fuehren.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Greedy-Pfad (temp=0)\n",
        "    def generate_path(tokens_start, temperature, max_steps):\n",
        "        tokens = tokens_start.copy()\n",
        "        path = []\n",
        "        for _ in range(max_steps):\n",
        "            with torch.no_grad():\n",
        "                context = tokens[-5:] if len(tokens) > 5 else tokens\n",
        "                logits = model(torch.tensor(context).unsqueeze(0))[0, -1, :]\n",
        "                probs = F.softmax(logits, dim=-1).numpy()\n",
        "\n",
        "                top2_idx = np.argsort(probs)[-2:][::-1]\n",
        "                top2_probs = probs[top2_idx]\n",
        "                gap = top2_probs[0] - top2_probs[1]\n",
        "\n",
        "                if temperature == 0:\n",
        "                    next_token = top2_idx[0]\n",
        "                else:\n",
        "                    scaled = logits / temperature\n",
        "                    next_token = torch.multinomial(F.softmax(scaled, dim=-1), 1).item()\n",
        "\n",
        "            word = tokenizer.idx_to_word.get(next_token, '?')\n",
        "            path.append({'word': word, 'gap': gap, 'prob': probs[next_token]})\n",
        "            tokens.append(next_token)\n",
        "\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "        return path\n",
        "\n",
        "    tokens_start = tokenizer.encode(text)\n",
        "\n",
        "    # Generiere mehrere Pfade mit leicht verschiedenen Temperatures\n",
        "    paths = {\n",
        "        'Greedy (temp=0)': generate_path(tokens_start, 0, max_steps),\n",
        "        'Sample 1 (temp=0.5)': generate_path(tokens_start, 0.5, max_steps),\n",
        "        'Sample 2 (temp=0.8)': generate_path(tokens_start, 0.8, max_steps),\n",
        "        'Sample 3 (temp=1.0)': generate_path(tokens_start, 1.0, max_steps),\n",
        "    }\n",
        "\n",
        "    # Visualisierung\n",
        "    fig, ax = plt.subplots(figsize=(16, 6))\n",
        "    colors = ['#2ecc71', '#3498db', '#e67e22', '#e74c3c']\n",
        "\n",
        "    for i, (label, path) in enumerate(paths.items()):\n",
        "        words = [text] + [p['word'] for p in path]\n",
        "        full_text = ' '.join(words)\n",
        "        y = len(paths) - i - 1\n",
        "\n",
        "        # Hintergrund\n",
        "        rect = FancyBboxPatch((-0.5, y - 0.35), 15.5, 0.7,\n",
        "                             boxstyle='round,pad=0.1',\n",
        "                             facecolor=colors[i], alpha=0.15, edgecolor=colors[i])\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Start-Text\n",
        "        ax.text(-0.3, y, text, fontsize=10, va='center', color='white', fontweight='bold')\n",
        "\n",
        "        # Generierte Tokens\n",
        "        x_pos = len(text.split()) * 1.5\n",
        "        for j, p in enumerate(path):\n",
        "            alpha = max(0.4, p['prob'])\n",
        "            ax.text(x_pos, y, p['word'], fontsize=10, va='center',\n",
        "                   color=colors[i], alpha=alpha, fontweight='bold')\n",
        "            x_pos += 1.5\n",
        "\n",
        "        ax.text(15.2, y, label, fontsize=8, va='center', color=colors[i], ha='right')\n",
        "\n",
        "    ax.set_xlim(-0.5, 15.5)\n",
        "    ax.set_ylim(-0.5, len(paths) - 0.5)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'Schmetterlingseffekt: Verschiedene Pfade ab \"{text}\"\\n'\n",
        "                 'Jede Abweichung fuehrt zu komplett anderem Folgetext',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_butterfly_effect(model, tokenizer, \"die katze\")\n",
        "visualize_butterfly_effect(model, tokenizer, \"der hund\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Praxis: Determinismus in Ollama & LM Studio\n",
        "\n",
        "### Ollama (Docker)\n",
        "\n",
        "```yaml\n",
        "# docker-compose.yml\n",
        "services:\n",
        "  ollama:\n",
        "    image: ollama/ollama:latest\n",
        "    ports:\n",
        "      - \"11434:11434\"\n",
        "    volumes:\n",
        "      - ollama_data:/root/.ollama\n",
        "    deploy:  # Fuer NVIDIA GPU – weglassen falls nur CPU\n",
        "      resources:\n",
        "        reservations:\n",
        "          devices:\n",
        "            - driver: nvidia\n",
        "              count: all\n",
        "              capabilities: [gpu]\n",
        "```\n",
        "\n",
        "**Determinismus-Parameter** (`temperature`, `seed`, `num_ctx`) werden **pro Request** gesetzt:\n",
        "\n",
        "```bash\n",
        "curl -s http://localhost:11434/api/chat -d '{\n",
        "  \"model\": \"qwen3:8b\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Was ist Docker?\"}],\n",
        "  \"options\": {\n",
        "    \"seed\": 123,\n",
        "    \"temperature\": 0,\n",
        "    \"num_ctx\": 2048\n",
        "  },\n",
        "  \"stream\": false\n",
        "}'\n",
        "```\n",
        "\n",
        "### LM Studio\n",
        "\n",
        "**`num_ctx` wird beim Laden des Modells fixiert** (nicht pro Request!):\n",
        "```bash\n",
        "lms load qwen3-8b --context-length 4096\n",
        "```\n",
        "\n",
        "`temperature` und `seed` werden im API-Request gesetzt (OpenAI-kompatibel):\n",
        "```bash\n",
        "curl -s http://localhost:1234/v1/chat/completions -d '{\n",
        "  \"model\": \"qwen3-8b\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Was ist Docker?\"}],\n",
        "  \"temperature\": 0,\n",
        "  \"seed\": 123,\n",
        "  \"stream\": false\n",
        "}'\n",
        "```\n",
        "\n",
        "### Vergleich\n",
        "\n",
        "| Parameter | Ollama | LM Studio |\n",
        "|---|---|---|\n",
        "| `temperature` | Per Request (`options.temperature`) | Per Request (`temperature`) |\n",
        "| `seed` | Per Request (`options.seed`) | Per Request (`seed`) |\n",
        "| `num_ctx` | **Per Request** (`options.num_ctx`) | **Beim Modell-Laden** (GUI/CLI) |\n",
        "| API-Stil | Ollama-native | OpenAI-kompatibel |\n",
        "| Default Port | 11434 | 1234 |\n",
        "\n",
        "### Hinweis: 100% Determinismus ist auf GPUs nicht moeglich\n",
        "\n",
        "Selbst bei perfekt identischen Einstellungen kann das **GPU-Thread-Scheduling**\n",
        "minimal andere Reihenfolgen produzieren. Das ist hardware-bedingt und nicht kontrollierbar.\n",
        "In der Praxis erreicht man damit ~99% Reproduzierbarkeit – bei den restlichen ~1%\n",
        "handelt es sich um Stellen, wo zwei Tokens fast identische Logits haben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Debugging mit logprobs\n",
        "\n",
        "Ollama (ab v0.12.11) kann **Log-Wahrscheinlichkeiten** zurueckgeben.\n",
        "Damit laesst sich die exakte Stelle finden, an der die Generierung divergiert.\n",
        "\n",
        "```bash\n",
        "curl -s http://localhost:11434/api/chat -d '{\n",
        "  \"model\": \"qwen3:8b\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Beschreibe The Matrix\"}],\n",
        "  \"options\": {\"seed\": 123, \"temperature\": 0, \"num_ctx\": 2048, \"num_predict\": 10},\n",
        "  \"logprobs\": true,\n",
        "  \"top_logprobs\": 5,\n",
        "  \"stream\": false\n",
        "}'\n",
        "```\n",
        "\n",
        "### Logprobs lesen\n",
        "\n",
        "Logprobs sind logarithmische Wahrscheinlichkeiten (`ln(p)`). Umrechnung:\n",
        "\n",
        "```\n",
        "logprob = -0.234  →  e^(-0.234) = 0.791  →  79.1%\n",
        "logprob = -2.891  →  e^(-2.891) = 0.056  →   5.6%\n",
        "```\n",
        "\n",
        "Je naeher an 0, desto sicherer das Modell. Wenn der **Gap** zwischen Top-1 und Top-2\n",
        "kleiner als ~0.001 ist, ist diese Stelle **anfaellig fuer Nichtdeterminismus**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simulate_logprobs(model, tokenizer, text, max_tokens=6):\n",
        "    \"\"\"Simuliert logprobs-Ausgabe wie bei Ollama, mit Stabilitaets-Analyse.\"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(text)\n",
        "    generated = text.split()\n",
        "\n",
        "    print(f'Prompt: \"{text}\"')\n",
        "    print(f'{\"Token\":<12} {\"logprob\":>8} {\"Prob\":>8} {\"Gap\":>8}  Status')\n",
        "    print('-' * 60)\n",
        "\n",
        "    logprob_data = []\n",
        "\n",
        "    for step in range(max_tokens):\n",
        "        with torch.no_grad():\n",
        "            context = tokens[-5:] if len(tokens) > 5 else tokens\n",
        "            logits = model(torch.tensor(context).unsqueeze(0))[0, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1).numpy()\n",
        "\n",
        "        top2_idx = np.argsort(probs)[-2:][::-1]\n",
        "        top1_prob = probs[top2_idx[0]]\n",
        "        top2_prob = probs[top2_idx[1]]\n",
        "        logprob = np.log(top1_prob + 1e-10)\n",
        "        gap = np.log(top1_prob + 1e-10) - np.log(top2_prob + 1e-10)\n",
        "\n",
        "        word = tokenizer.idx_to_word.get(top2_idx[0], '?')\n",
        "        word2 = tokenizer.idx_to_word.get(top2_idx[1], '?')\n",
        "\n",
        "        stability = 'STABIL' if gap > 0.5 else 'INSTABIL' if gap < 0.1 else 'GRENZWERTIG'\n",
        "        marker = '  ' if gap > 0.5 else ' !' if gap < 0.1 else ' ~'\n",
        "\n",
        "        print(f'{word:<12} {logprob:>8.4f} {top1_prob*100:>7.1f}% {gap:>8.4f} {marker} {stability}')\n",
        "\n",
        "        logprob_data.append({\n",
        "            'word': word, 'word2': word2,\n",
        "            'prob1': top1_prob, 'prob2': top2_prob,\n",
        "            'gap': gap,\n",
        "        })\n",
        "\n",
        "        tokens.append(top2_idx[0])\n",
        "        generated.append(word)\n",
        "\n",
        "        if word == '<EOS>':\n",
        "            break\n",
        "\n",
        "    print(f'\\nGeneriert: \"{\" \".join(generated)}\"')\n",
        "\n",
        "    # Visualisierung: Gap pro Schritt\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "    steps = range(len(logprob_data))\n",
        "    gaps = [d['gap'] for d in logprob_data]\n",
        "    words_gen = [d['word'] for d in logprob_data]\n",
        "    probs1 = [d['prob1'] * 100 for d in logprob_data]\n",
        "    probs2 = [d['prob2'] * 100 for d in logprob_data]\n",
        "\n",
        "    # 1. Wahrscheinlichkeiten Top-1 vs Top-2\n",
        "    ax1.bar([s - 0.15 for s in steps], probs1, 0.3, label='Top-1', color='#2ecc71')\n",
        "    ax1.bar([s + 0.15 for s in steps], probs2, 0.3, label='Top-2', color='#e74c3c', alpha=0.7)\n",
        "    ax1.set_ylabel('Wahrscheinlichkeit (%)')\n",
        "    ax1.set_title('Top-1 vs Top-2 Wahrscheinlichkeit pro Schritt')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. Gap (Stabilitaet)\n",
        "    gap_colors = ['#2ecc71' if g > 0.5 else '#e74c3c' if g < 0.1 else '#f39c12' for g in gaps]\n",
        "    ax2.bar(steps, gaps, color=gap_colors)\n",
        "    ax2.axhline(y=0.5, color='#2ecc71', linestyle='--', alpha=0.5, label='Stabil (>0.5)')\n",
        "    ax2.axhline(y=0.1, color='#e74c3c', linestyle='--', alpha=0.5, label='Instabil (<0.1)')\n",
        "    ax2.set_ylabel('Logprob-Gap')\n",
        "    ax2.set_xlabel('Generierungsschritt')\n",
        "    ax2.set_title('Stabilitaet der Token-Wahl (Gap zwischen Top-1 und Top-2)')\n",
        "    ax2.set_xticks(list(steps))\n",
        "    ax2.set_xticklabels(words_gen, rotation=45, ha='right')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.suptitle(f'Logprob-Analyse: \"{text}\"', fontsize=13)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "simulate_logprobs(model, tokenizer, \"die katze\")\n",
        "simulate_logprobs(model, tokenizer, \"der hund\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Zusammenfassung: Kette der Determinismus-Bedingungen\n",
        "\n",
        "```\n",
        "Fester num_ctx\n",
        "    → identisches Speicher-Layout\n",
        "        → identische Tile-Aufteilung (Flash Attention Blocks)\n",
        "            → identische Reduktionsbaum-Struktur\n",
        "                → identische Float-Rundung\n",
        "\n",
        "Fester seed\n",
        "    → identischer Zufallsgenerator-Zustand\n",
        "\n",
        "temperature = 0\n",
        "    → Greedy Decoding (kein Sampling)\n",
        "\n",
        "Alle drei zusammen → identische Logits → identische Token-Wahl → identischer Output\n",
        "```\n",
        "\n",
        "**Restrisiko**: GPU-Thread-Scheduling ist nicht-deterministisch.\n",
        "In ~1% der Faelle koennen zwei Tokens fast identische Logits haben\n",
        "und die Rangfolge kippt. Das ist hardware-bedingt und nicht vermeidbar.\n",
        "\n",
        "### AI SDK 6 Integration\n",
        "\n",
        "```typescript\n",
        "// ai-sdk-ollama: num_ctx direkt in den Model-Options\n",
        "import { createOllama } from 'ai-sdk-ollama';\n",
        "import { generateText } from 'ai';\n",
        "\n",
        "const ollama = createOllama({ baseURL: process.env.LLM_PROVIDER_URL });\n",
        "\n",
        "const { text } = await generateText({\n",
        "  model: ollama('qwen3:8b', { options: { num_ctx: 2048 } }),\n",
        "  temperature: 0,\n",
        "  seed: 123,\n",
        "  prompt: 'Was ist Docker?',\n",
        "});\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def summary_visualization():\n",
        "    \"\"\"Zusammenfassung als Infografik.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.set_xlim(0, 14)\n",
        "    ax.set_ylim(0, 10)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Titel\n",
        "    ax.text(7, 9.5, 'Determinismus-Rezept fuer lokale LLMs',\n",
        "           ha='center', fontsize=16, fontweight='bold', color='white')\n",
        "\n",
        "    # Drei Saeulen\n",
        "    pillars = [\n",
        "        (2.5, '#e74c3c', 'temperature = 0',\n",
        "         'Greedy Decoding:\\nImmer das Token\\nmit dem hoechsten\\nLogit-Wert',\n",
        "         'Entfernt\\nZufaelligkeit'),\n",
        "        (7.0, '#3498db', 'seed = 123',\n",
        "         'Fixer Random State:\\nIdentischer\\nZufallsgenerator\\nbei jedem Run',\n",
        "         'Fixiert\\nSampling'),\n",
        "        (11.5, '#2ecc71', 'num_ctx = 2048',\n",
        "         'Festes Context-Window:\\nIdentisches Speicher-\\nLayout, identische\\nTile-Aufteilung',\n",
        "         'Fixiert\\nFloat-Berechnung'),\n",
        "    ]\n",
        "\n",
        "    for x, color, param, desc, effect in pillars:\n",
        "        # Parameter-Box\n",
        "        rect = FancyBboxPatch((x - 1.8, 7.2), 3.6, 1.2,\n",
        "                             boxstyle='round,pad=0.15',\n",
        "                             facecolor=color, edgecolor='white', alpha=0.9)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, 7.8, param, ha='center', va='center',\n",
        "               fontsize=11, fontweight='bold', color='white', family='monospace')\n",
        "\n",
        "        # Pfeil\n",
        "        ax.annotate('', xy=(x, 6.0), xytext=(x, 7.2),\n",
        "                   arrowprops=dict(arrowstyle='->', color='white', lw=2))\n",
        "\n",
        "        # Beschreibung\n",
        "        rect2 = FancyBboxPatch((x - 1.8, 3.5), 3.6, 2.5,\n",
        "                              boxstyle='round,pad=0.15',\n",
        "                              facecolor=color, edgecolor='white', alpha=0.2)\n",
        "        ax.add_patch(rect2)\n",
        "        ax.text(x, 4.8, desc, ha='center', va='center',\n",
        "               fontsize=9, color='white')\n",
        "\n",
        "        # Effekt\n",
        "        ax.annotate('', xy=(x, 2.3), xytext=(x, 3.5),\n",
        "                   arrowprops=dict(arrowstyle='->', color='white', lw=2))\n",
        "        ax.text(x, 1.8, effect, ha='center', va='center',\n",
        "               fontsize=10, color=color, fontweight='bold')\n",
        "\n",
        "    # Ergebnis\n",
        "    rect_result = FancyBboxPatch((2, 0.2), 10, 1.0,\n",
        "                                boxstyle='round,pad=0.15',\n",
        "                                facecolor='#f39c12', edgecolor='white', alpha=0.9)\n",
        "    ax.add_patch(rect_result)\n",
        "    ax.text(7, 0.7, 'Identische Logits → Identische Token-Wahl → Identischer Output (~99%)',\n",
        "           ha='center', va='center', fontsize=11, fontweight='bold', color='black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "summary_visualization()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
