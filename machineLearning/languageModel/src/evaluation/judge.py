"""
LLM-as-a-Judge: Core evaluation logic
=======================================

Evaluates generated texts from a MiniGPT model on three criteria:
1. Grammatik (1-5)
2. Kohaerenz (1-5)
3. Relevanz (1-5)

The judge LLM (e.g. Qwen3 via Ollama/LM Studio) returns a JSON response
containing scores and a reasoning string.
"""

from evaluation.judge_config import EvaluationScore
from evaluation.llm_client import call_judge_llm


# =============================================================================
# JUDGE PROMPTS
# =============================================================================

JUDGE_SYSTEM_PROMPT = """\
Du bist ein Bewertungs-Assistent fuer maschinell generierte deutsche Texte.

KONTEXT: Die Texte stammen von einem sehr kleinen Sprachmodell (MiniGPT, ~100k Parameter),
das auf einem winzigen deutschen Korpus trainiert wurde. Die Saetze sind kurz (5-10 Woerter).
Bewerte fair fuer dieses Niveau â€” perfekte Grammatik oder tiefe Semantik sind nicht zu erwarten.

Bewerte jeden Text auf drei Kriterien (je 1-5):

1. **grammatik_score** (1-5):
   1 = Voellig ungrammatisch, nur Wortsalat
   2 = Schwere Grammatikfehler, kaum lesbar
   3 = Erkennbare Struktur, aber deutliche Fehler
   4 = Weitgehend korrekte Grammatik, kleine Fehler
   5 = Grammatisch korrekt fuer dieses einfache Niveau

2. **kohaerenz_score** (1-5):
   1 = Kein Zusammenhang zwischen den Woertern
   2 = Einzelne Woerter passen, aber kein sinnvoller Satz
   3 = Ansatzweise sinnvoll, aber logische Brueche
   4 = Groesstenteils zusammenhaengend und sinnvoll
   5 = Klarer, zusammenhaengender Satz

3. **relevanz_score** (1-5):
   1 = Output hat keinen Bezug zum Prompt
   2 = Sehr entfernter Bezug
   3 = Teilweise relevant, schweift aber ab
   4 = Groesstenteils relevant zum Prompt
   5 = Direkt relevant, sinnvolle Fortsetzung

Antworte NUR mit einem JSON-Objekt in genau diesem Format:
{
    "grammatik_score": <1-5>,
    "kohaerenz_score": <1-5>,
    "relevanz_score": <1-5>,
    "begruendung": "<kurze Begruendung auf Deutsch>"
}
"""

JUDGE_USER_PROMPT_TEMPLATE = """\
Bewerte den folgenden generierten Text:

**Prompt (Eingabe):** {prompt}
**Generierter Text:** {generated_text}
"""


# =============================================================================
# EVALUATION
# =============================================================================

def evaluate_single_output(prompt: str, generated_text: str) -> EvaluationScore | None:
    """
    Evaluate a single generated text using the judge LLM.

    Args:
        prompt: The input prompt given to the model.
        generated_text: The text generated by MiniGPT.

    Returns:
        EvaluationScore on success, None on failure.
    """
    user_prompt = JUDGE_USER_PROMPT_TEMPLATE.format(
        prompt=prompt,
        generated_text=generated_text,
    )

    # First attempt
    try:
        result = call_judge_llm(JUDGE_SYSTEM_PROMPT, user_prompt)
        return EvaluationScore.from_dict(result)
    except Exception as e:
        print(f"   [!] First attempt failed: {e}")

    # Retry
    try:
        result = call_judge_llm(JUDGE_SYSTEM_PROMPT, user_prompt)
        return EvaluationScore.from_dict(result)
    except Exception as e:
        print(f"   [X] Evaluation failed after retry: {e}")
        return None
